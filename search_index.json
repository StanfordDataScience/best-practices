[["index.html", "Open, rigorous and reproducible research: A practitioner’s handbook Preface 0.1 Authors 0.2 Acknowledgements", " Open, rigorous and reproducible research: A practitioner’s handbook Dallas Card, Yan Min, Stylianos Serghiou 2021-02-25 Preface This book starts from the premise that there is a lot we can all do to increase the benefits of research. To begin, let’s consider the main limitations with research that is not carried out and shared in an open, transparent, and reproducible way: If papers are published in venues that are only available to those who pay for access, the vast majority of the world will not be able to see the output of all the work that went into producing it; this limits the potential reach and benefit to others. Because of the complexity involved in many analyses, it is nearly impossible to describe every detail and choice that went into an analysis in the main paper; without accompanying code, it can be very very difficult for others to be certain about exactly what was done. Even if code is made available, there can be additional challenges to reproducing or re-analyzing past work, such as inaccessible data or deprecated software. If others are not able to easily re-analyze past work, that limits the ability of the community to explore other analysis pathways, combine datasets, attempt to generalize experiments to new settings, etc. If experiments are carried out without proper care in experiment design and analysis, there are likely to be more erroneous findings in the literature, making it harder for everyone to make sense of the object of study. The more that new researchers have to wade through results that may not be credible, they more they are delayed from making genuine advances Of course, there are numerous reasons why people don’t put more effort into making their work open, transparent, and reproducible: Perhaps most importantly, doing so does require some additional work, and current incentive structures do not necessarily reward these efforts; however, this is changing in many fields, and certain communities place a lot of value on such things. Moreover, the cost of mistakes can be high, and this sort of openness helps to avoid them. Some data is legitimately not possible to share, due to concerns about privacy, copyright, or other considerations. Using such data will generally be less useful to the world than using more open data, but some work will of course require it. However, there are still things that can be done to avoid the worst problems, including being transparent about the analyses carried out, the protocol for collecting data, and other techniques such as pre-registration, which can bolster people’s confidence in a piece of work. Many people worry that making their data and code open to the world will expose them to risk or ridicule, either because they fear they have made mistakes, or they think it will reveal them to be a poor coder. This is understandable, but generally misplaced. It is better to catch errors early. Moreover, most people will be happy if you share any code, no matter how bad it is, and doing so is one of the best ways to improve, especially if you begin with the end in mind. Finally, many people don’t know where to start. Most guides to open science and reproducibility take the form of complete books or corpus, and try to teach an entire philosophy and comprehensive approach to research, which can be overwhelming. In this document, we take a different approach. Our main goal here is to show how there are many ways to make your research more open, transparent, and reproducible on the margin, and that each step in that direction may bring some benefit. While there will always be nuances and requirements specific to each field, in general there is a great deal that we can learn from each other, and most ideas can be applied to any domain. In summary, this handbook is a guide to making science more open, transparent, and reproducible by presenting best practices in a way that is: modular: individual ideas can be used separately or combined practical: focused on the most tractable and impactful practices general: applicable to any field that works with data and statistical analysis concise: aimed at the busy scientists who doesn’t have time to take a full course right now We break this guide down into three mains sections. Each section contains many modular components, each of which can be considered and used independently or in combination with the others: Section 1: Careful study design to help ensure and demonstrate that results and conclusions are valid and useful: Thoughtful determination of experimental parameters, such as using power analysis to estimate an appropriate sample size Distinguishing between exploratory and confirmatory research Pre-analysis planning of statistical analyses Ensuring that all relevant data is collected in order to be comparable with past work Additional considerations, such as pre-registration, planning for potential problems, and consideration of ethical implications. Section 2: Adopting best practices in analyzing data and reporting results: Preliminary: decisions and considerations before working with any data. Statistical analysis plan: plan your analytic approach beforehand. Data generation: generate an appropriate set of data. Data preparation: transparently prepare your data for data analysis. Data visualization: visualize all data using informative visualizations. Data summarization: summarize all data using appropriate statistics. Data analysis: analyze all data and avoid common blunders. Data analysis - medicine: a few more considerations for medical research. Statistical analysis report: report transparently and comprehensively. Examples: published literature exemplifying principles of this manual. Section 3: Making relevant research materials available to all: Open Data: making the raw data available for further research and replication Open Source Code: making the analysis pipeline transparent and available for others to borrow or verify Reproducible Environments: making not just the data and code available for others, but making it easy for them to re-run the analysis in an easily reproducible manner Open Publication Models such that anyone can see the scholarly output associated with the work Documenting Processes and Decisions: making it clear to interested parties not only what was done and how, but also why, by mechanisms such as open lab notebooks In addition, appendices cover additional resources, such as frequently asked questions, discipline-specific considerations and linked to additional resources (of which there are plenty!) 0.1 Authors Dallas Card is a postdoctoral scholar with the Stanford NLP Group and the Stanford Data Science Institute. He received his Ph.D. from the Machine Learning Department at Carnegie Mellon University. Yan Min is a Ph.D. candidate working with Mike Baiocchi as part of the Department of Epidemiology and Population Health at Stanford University. She has previously completed her medical studies in China. Stylianos (Stelios) Serghiou is an AI Resident at Google Health working on using modern methods of data science to empower patients, doctors and clinical researchers. He received his Ph.D. in Epidemiology and Clinical Research and Masters in Statistics from Stanford University, where he was advised by John Ioannidis. He previously completed his medical training at the University of Edinburgh, UK. 0.2 Acknowledgements We would like to thank all early readers of this work, who’s feedback we sincerely appreciate. We are especially thankful to the Stanford Data Science Initiative community, Russ Poldrack, John Chambers and Steve Goodman. "],["study-design-phase.html", "1 Study design phase 1.1 Define the research question 1.2 Choose Your Study Design 1.3 Study Designs 1.4 Recognize Different Sources of Errors/Uncertainties in Estimation 1.5 Typical Biases in Different Data Sources and Study Designs 1.6 How to Reduce and Estimate Different Types of Errors 1.7 Create Your Analytic Plan 1.8 Start Documenting Your Grand Study", " 1 Study design phase It is always exciting to start a new research project. By the time you actually roll-up your sleeves and get your hands dirty, you have probably been pondering on many things that related to your new project: what kind of questions would I like to answer? How should I formalize the question? How do I get to the answer? Should I conduct an experiment to gather some data, or should I explore the existing datasets? What does the data look like anyway? How do I make sure my answer is not wildly wrong? Then you find yourself buried in those questions, do not know where to start… Well, firstly, you are definitely on the right track thinking about all those questions! Congratulations, you just have just taken the first step! Secondly, do not be intimidated by this big fuzzy ball of thoughts. You may not know where to start, how to start, or whether you have thoroughly considered everything to start your research project. This is totally normal! I am here to propose a general framework to get your work started. Let’s call it “the research project starter kit”! In the following four chapters, I will be introducing concepts, tools, and easy-to-follow frameworks as part of this precious starter kit. I will also share insights on study designs from experienced researchers in different scientific fields. Chapter 1 will discuss how to define the research question, a clear and effective one that we can actually act upon. Chapter 2 will introduce study designs techniques that can answer the research question effectively. Chapter 3 will touch on how to create a realistic analytic plan. Finally, Chapter 4 will provide tools for documenting all the research planning steps. [add a paragraph] How does the best practice relate to open science? Without well planned study design, open science might be simply unfeasible as you cannot go back in time to retrieve all missing information. 1.1 Define the research question 1.1.1 Start with a question in mind A well-defined research question reflects the researcher’s careful thinking of the problem that he/she/they is trying to tackle. Specifying a good research question also serves the researcher a long way: Provides clear aims of what to achieve through the study Sets reasonable expectations and future goals Helps select appropriate methodology moving forward Gives a better chance to practice open science At this point, you may think, “oh, come on, man! A seasoned researcher like me, of course, knows how to come up with a good research question!” Well, I would say, “Man, think twice!” So what does a well-defined research question look like anyway? To answer this question, we shall consider the following two aspects: scientific contribution to the field and operational feasibility. To evaluate the scientific contribution: Does the question have a solid scientific base? Is the question novel? If the question is sufficiently answered, what will it add to the current knowledge? To evaluate the operational feasibility: What is the study unit to answer the question? (Individuals? Microbial - colonies? Countries? Planet or celestial systems? Does the question include an explicit intervention / treatment / exposure? Does the question imply a comparison group? Is there an anticipated outcome? Note: Since academic fields vary, it does not mean that your question has to fulfill all these points mentioned above. However, we do encourage you to go through these questions while you are contemplating the research question. Figure 1.1: Figure credit: https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/ 1.1.2 Classification of different questions Another helpful practice is to carefully scrutinize what kind of question you are asking. It does not mean that some types of questions are absolutely superior to the others. The purpose of thinking through the type of the question allows us to be true to ourselves and honest to our audience when we are making any inferences from our research. Now I will go through two general classifications of reserved questions. Confirmatory vs. Exploratory Confirmatory questions typically involve a set of hypotheses, a null hypothesis with one or more alternatives. It often requires the researcher to develop a concrete research design to test these hypotheses. The question is often answered via inductive inference. Such inference is often considered as “strong inference”, and is deemed to make a “scientific step forward”. (Platt 1964) Some examples of confirmatory questions include… add examples! add reading on inductive and deductive reasoning (maybe the one from Steve’s class) Exploratory questions, unlike confirmatory questions, are not explicitly hypothesis driven. Rather, these questions are often considered to be hypotheses-generating. Hence, exploratory questions do not mean to achieve “strong inference”. Results from exploratory research cannot be over-interpreted as something confirmatory, and often yield a higher false positive rate. However, exploratory questions are meaningful and necessary for new discoveries and unexplored topics. Before we make any “strong inference”, we should always attempt to validate the results from exploratory research in a confirmatory setting. Some examples of exploratory questions include… add examples! In summary, after you carefully think through your research question, it will be more clear to see whether your question is rather hypothesis driven or hypothesis generating. Either way may make interesting research topics, as long as you are making the right amount of inference from the results. Please be true to yourself as the Knights of the Round Table to King Arthur! When you ask an exploratory question, please do not pretend it is confirmatory no matter how hard you want to believe it is confirmatory. When you think you are asking a confirmatory question, make sure it is really confirmatory, not an exploratory question with fancy confirmatory wordings. Believe me, your reviewers will be able to tell! Causal vs. Non-Causal Another lens of classifying the research question is to examine whether the question is trying to draw a conclusion about a causal relationship between the indexed exposures and outcomes. As a typical graduate student conducting research, I often find myself either busy establishing an association, or busy determining whether the association I find involves a cause and its effect on the outcome. Although even the famous American writer John Barth once said “The world is richer in associations than meanings, and it is the part of wisdom to differentiate the two.”, this is rather a post-hoc strategy. As having been stressed multiple times, forward thinking is really the key to high quality research. When you have a research question in mind, while thinking about how brilliant your idea is, please also go through the following items to see whether your question is causal or not. The typical causal question includes the following components: A well defined cause (what can be qualified as a cause? Still debatable, expand). A well defined outcome. A scientifically plausible effect on the outcome that can be attributable to the cause. Provide a better list of components. Moreover, English epidemiologist and statistician Austin Bradford Hill carefully summarized the common characteristics of causal relationship in his paper [The Environment and Disease: Association or Causation?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1898525/?page=1 in 1965. Although this paper later was referred to as the “Hill Causal Criteria”, Hill himself actually suggested that we should take all these characteristics with a grain of salt. Most of the causal questions contain the “C” word ( C for cause) or the “W” word (W for why). However, a lot of times, causal questions does not explicitly contain the word “cause”, rather, it describes the effect that one factor A has on another factor B. For example: What causes urban residential segregation? Why are rural people more politically conservative than urban people in the - United States? How effective hydroxychloroquine is in treating COVID-19 patients? How does the concentration of silver nitrate affect the formation of silver crystals? The two typical classes of non-causal questions we encounter are descriptive questions and observational/associational questions. The former primarily describes the objective existence of certain phenomena. (e.g what is the ductility of bronze, silver, gold?) The latter concerns the relationship between two factors without considering the underlying causal mechanisms. (e.g how does metal ductility relate to its melting point?) Design an infographic guide for people to go through confirmatory / exploratory and causal / non-causal questions. Note: The takeaway here is that knowing the type of questions that you are asking comes quite handy when determining the downstream analytical methodology and guarding the proper inference! 1.1.3 Not All Research Questions Are Hypotheses “Every Science begins as philosophy and ends as art; it arises in hypothesis and flows into achievement.” Will Durant This section, I would like to further emphasize the characteristics of research hypotheses, as they are the driving force for confirmatory studies and oftentimes the “products” of exploratory studies. As mentioned previously, research questions, as a more general concept, can take on myriad forms with fewer requirements and restrictions; whereas hypotheses, as a subset of research questions, are often phrased in a more specific way with at least one priori belief and one or more alternative(s). It usually does not take on a question form, rather it is a statement, an educated, testable prediction about the results. The main criteria of a good research hypothesis include: Written in clear and simple language that clearly defines both the dependent - and independent variables. States the priori belief of the relationship between the dependent and - independent variables. Variables are defined without scientific ambiguity and are feasible to measure - using either a quantitative or qualitative approach. The hypothesis must be testable using scientific methods. (While constructing the hypothesis, one shall try to think of different methods that might be applicable to test your hypothesis). The hypothesis is also feasible with respect to the current resources and time frame. Here are some examples of hypotheses: Farmed salmon in Norway is more likely to have a higher prevalence of parasitic diseases than the wild salmon. (Good!) My comment: The dependent variable is the prevalence of parasitic diseases. Independent variable is the status of farmed or wild. The predicted effect here is farmed salmon - higher prevalence of parasitic disease. Here the effect is prevalence, which is unambiguous. This is reasonably straightforward to test as well. The extinction of honey will lead to mass extinction to other species, including humans. (Poor!) Now you try to apply the main criteria of a good hypothesis to critique this hypothesis, why does it sound plausible, yet is such a poor hypothesis? Some fields have their own guidelines on how to generate “tight” hypotheses. For example, the P.I.C.O framework is commonly used in evidence based medicine to formulate high quality clinical questions. The following table summarizes each P.I.C.O component. Although this framework is designed for one particular field, it could be applicable to other scientific disciplines as well. If your research question can be formulated in such a comparison setting, please think through these four components. Table 1.1: The PICO framework Abbreviation Definition Description P Patients, Population How would you describe the patients or population that I am interested in studying? I Intervention, Exposure What is the main treatment, intervention, or exposure of my study? C Comparison What is the alternative treatment, intervention, or exposure that I would like to compare to the main intervention? O Outcome What is the outcome, measurements, or affect of the intervention and the alternative intervention? Protip: Use study reporting guidelines to navigate your research question formulation process. When starting a new research project, despite our enthusiasm and motivation, we may still feel quite clueless, especially for us young researchers. Firstly, if you feel this way, fear no more, you are not alone. Secondly, there might be some good news for you. Depending on your study design (coming up in the next chapter), there are corresponding protocols to guide researchers through the study reporting phase. These reporting protocols provide a list of necessary information needed to ensure the transparency of the reported study. These reporting protocols are often developed by a panel of experts within the research field. They can be used to spur high-quality research question/hypothesis generating even at the early stage of a study! Here are several examples of such reporting guidelines: Consolidated Standards of Reporting Trials (CONSORT) for randomized controlled trials Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) for systematic reviews and meta-analyses Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) for observational studies, such as cohort, case-control, and cross-sectional studies, and conference abstracts for these studies Case Report Guidelines (CARE) for case reports Consolidated Criteria for Reporting Qualitative Research (CPREQ) for qualitative research Animal Research: Reporting of In Vivo Experiments Guidelines (ARRIVE) for research involving experient animals Consolidated Health Economic Evaluation Reporting Standards (CHEERS) for economic evaluations Protip: Conduct literature review prior to formulate or finalize the research question or hypothesis. In Star Trek Voyager, captain Janeway led her crew to explore the uncharted delta quadrant. (They came from the alpha quadrant.) One obvious reason that they constantly got into trouble (so the show could last for seven seasons) was that they lacked the knowledge of all the planets and new alien species they were dealing with. They called them explorers. And their trip back home was full of treacherous adventures. Trust me, my friend, you don’t want your research to be anything like an adventure! Sufficient literature review prior to formulating or finalizing your research question / hypothesis will provide you a map of the scientific field that you are interested in exploring. 1.1.4 Thinking Forward vs. Thinking Backward “By failing to prepare, you are preparing to fail.” Benjamin Franklin By now, you may have noticed that the main idea we would like to drive home is “Think ahead of time! Plan ahead of time! Prepare ahead of time!”. Forward thinking in research conduct allows the researchers not only to better define and understand the research topics themselves, but also anticipate potential contingencies and prepare to tackle the expected “unexpected”. Backward thinking in scientific research is not uncommon. Think about the following scenario: you worked hard to generate some seemingly meaningful results. Because you didn’t form any plausible research question, and you were too lazy to conduct a literature review, now you do not know how to interpret the results. “Hey, it’s not too late to start some literature review!”, you said to yourself. Then you put the results in the google search bar and added a question mark in the end. Then you naively think “Voila, problem solved!”. You continued on writing the discussion section of your paper… Please don’t feel ashamed if this situation sounds quite familiar to you, however, you must know by now that this kind of practice is just horrible, period! Backward thinking often leads to HARKing, which stands for Hypothesis After Result is Known. HARKing commonly increases the risk of type I error, and further leads to reproducibility crisis. (Click this Link to Read more about HARKing.) 1.1.5 Fun Readings and Additional Resources Tukey, Design Thinking, and Better Questions - a neat blog article on research questions written by Roger Peng from Johns Hopkins University. 1.2 Choose Your Study Design 1.2.1 But First, Know Your Data Where does data come from anyway? In general there are three main sources of data for your research: Data gathered by researchers themselves. In this case, the researchers are more likely to have a better understanding of the data used in their study, as they are actively involved in the data collecting procedure. Some examples of this kind of “shoe-leather” work include epidemiologists gathering patient-level data from a clinical trial; anthropologist recording interviews of residents from a certain tribe, political scientists conducting polling survey online, earth science researchers collecting information on soil and weather conditions, electrical engineers simulating a target signal of interest, etc. Single-source pre-existing data. In this case, the data has already been gathered mostly for either a more general purpose or a different purpose. But all the information has been consolidated into one dataset, and it is ready to be repurposed for the researchers’ new study. Some examples include bioinformaticians using a subset of U.K biobank data for genetic analysis; nutritionists using a pre-established cohort, such as Stanford Well Living Laboratory to investigate alcohol consumption; econometricians employing Uber driver data to evaluate service quality, etc. Multi-source pre-existing data. In this scenario, researchers need to pull data from multiple existing data sources. The original data sources can be passively collected, such as insurance claims, Facebook user information, or, actively gather data with a study design in place… Researchers have to harmonize the data from different sources to fit the goal of the current study. Some examples are using health insurance claims, hospital registry, and surgeon records to evaluate healthcare quality; employing satellite images and other meteorological measurements to study crops’ growing pattern; Once we obtain the research data, I don’t know about you, but I get extremely excited! However, before we load everything into any analytic software while humming our favorite tunes, there are several sanity checks we should go over together: How are the study units selected? (Sampling scheme applied? Administrative information?…) How is the data collected? (Survey? Interview? Wearable Devices?…) Are there any underlying data structures that we should be aware of? (Correlated study units? Repeated measurements? …) Carefully going through these questions will help us anticipate potential biases, choose appropriate analytical methodologies, draw coherent inferences and conclusions, and make planned extrapolations. The following section will point out all the concepts that we need to answer the above questions. In this manual, I will only scratch the surface of these concepts, but provide links to further readings for the keeners! 1.3 Study Designs “We must never make experiments to confirm our ideas, but simply to control them.” Claude Bernard 1.3.1 What is study design anyway? - An Important Mind Map Now, let’s think about planning an epic expedition trip to Yosemite. You want to know the general terrain and geology that you will set your feet on, choose the route that takes you to the Half Dome, bring the right gear to excel your performance. And if you have a bit of extra time, you might also want to test the gear during a shorter hike. Once you reach the top, you take breathtaking photos. Then you come back, tell all your family and friends about how you made it to the top in great details over beer. If this sounds familiar, congratulations, you are a natural in study design. Similar to planning an epic hiking trip, the broader concept of study design is the process of planning the execution protocol and analytic method of your study. By the time you have got a clearly defined research question, the nature of your research question should hint you towards certain study designs, or at least it should help you toss some of the designs into the dumpster. The following section will provide some ideas on how to use the type of questions you are trying to answer to guide you through choosing the appropriate study design. For now, let’s first go through the components of study design: Execution Protocol is the core of study design. Most of the time when people talk about study design, they refer to this study execution protocol. Depending on the stage of the research, the protocol may include an array of documentations: the detailed plan on fieldworks to gather population data (human population, microbe population, animals, plants, … ), including the sampling scheme, recruitment strategy, quality control, etc; if the research is related to wet lab, then the protocol shall document all the steps that have been done in the experiments, including the equipment used to process these steps; if human participants are involved, one should also compose the Institutional Review Board (IRB) protocol. If you are using multi-sourced data, the protocol shall elaborate on the data harmonization from different sources. Even if you are using a single-sourced dataset, your inclusion and exclusion criteria could differ from the original data collecting protocol. Hence, in your protocol, you shall document any inclusion or exclusion of records. This will make sure when you are trying to rerun your study, or others are trying to reproduce your study, you will all be on the same page prior to conducting any analysis. To sum up, whatever you do to gather your data, to massage your data prior to the analysis, document all your steps in the execution protocol. Note: Sampling is totally art. The study design should also govern how the study units are sampled either directly via fieldwork, bench work, or from existing data sources. We will lightly touch on this topic when introducing the types of study designs. Analytic Method should also naturally stem from the kind of question you are asking. Moreover, a lot of times the execution protocol implies a proper set of analytical methods. You may “play around” with your data and get yourself familiarized. We all do that. However, once you decide to take it seriously, plan your analysis prior to generating any results. For example, if your study involves a hypothesis, you shall predetermine how many alternative hypotheses you would like to test. If you are doing a confirmatory study and would like to interpret the effect sizes of independent variables on the dependent variable, you may consider a method that can actually obtain the effect estimator, instead of using some non-parametric method or black-box method, where the effect sizes are not explicit. If you are asking a causal question, the analytical method should be able to fit into the causal inference framework. Protip: Another great practice is that when you think through all the models picture the following figure, where the x-axis is the model complexity, the y-axis is the number of assumptions of the data the model makes. The fuzziness of the ball indicates the interpretability of the model. Most of the time, there is no perfect model to answer your question, you end up evaluating the tradeoffs among the available methods. This is where knowing your data comes quite handy. Protip: Another great practice is that when you think through all the models picture the following figure, where the x-axis is the model complexity, the y-axis is the number of assumptions of the data the model makes. The fuzziness of the ball indicates the interpretability of the model. Most of the time, there is no perfect model to answer your question, you end up evaluating the tradeoffs among the available methods. This is where knowing your data comes quite handy. Figure 1.2: Figure credit: Yan Min Protip: If your study involves human subjects, you will need to draft an IRB protocol for approval. Although the purpose of the IRB is not to govern the study quality and rigor but to guarantee the ethical conducts of the research, some of the IRB frameworks can be used as a guiding tool to think through the study execution. Here is the link to the Stanford IRB office, you may find a lot more resources from their website. 1.3.2 Types of Study Designs The concept of study design exists in almost every academic field. Some study designs are more common in one field than the others; some are widely used across the disciplines but with esoteric names in certain fields. To avoid confusion, I will first discuss the general study design patterns with respect to the question types, then introduce an additional factor - temporality. I will mention specific study design names commonly used in my discipline (Epidemiology) in several examples, but I will provide the definition of each study design type. Note: When do you need a study design? If you don’t want to turn your research project into an unwanted surprise, you will always be in need of a study design! 1.4 Recognize Different Sources of Errors/Uncertainties in Estimation “The mistake is thinking that there can be an antidote to the uncertainty” David Levithan As long as you conduct scientific research, it is just absolutely inevitable for you to deal with errors/uncertainties of some sorts. However, don’t panic! Although there is no antidote to the uncertainties, anticipating and knowing what kind of error or uncertainty you are dealing with is necessary, so that you can guard your research from being shipwrecked. Different study designs and data types have different intrinsic errors that could reduce the validity or/and accuracy of your study. The following section will, first, introduce different types of errors; then, introduce how different errors can compromise a study; further, provide examples of intrinsic errors for different study designs and data sources; finally, briefly touch on different methods we can use to evaluate the errors/uncertainties. 1.4.1 Types of Errors Generally speaking, there are three types of errors, namely random error, systematic error, and blunders. The following diagram indicates the classification of errors. Figure 1.3: Figure credit: Yan Min Random error (A.K.A unsystematic error) often refers to random noise or random fluctuations that are impossible to remove (irreducible) when taking the measurements. Such errors are usually unpredictable. The exact error cannot be replicated by repeating the measurements again. Systematic error (A.K.A systematic bias) usually results from flaws in the execution of a study, such as the selection of study units, the procedure for gathering relevant information. The error is consistent overtime, if the flaw in the study is not corrected or adjusted. Blunders are just straight mistakes in the process of research conduct. This is unfortunate. However, as long as these mistakes are spotted, it is often easy to correct. We will not spend additional effort to discuss blunders as the previous two types of errors are the “boat-sinkers” in more than 99% of time. Among all three types of errors, the systematic error is the most complicated and can completely nullify the study results. Hence, let’s take a further look at it. As shown in the diagram above, systematic error can be further classified into selection bias, information bias, and confounding bias. Selection Bias occurs when a systematic error in the inclusion or exclusion of study units happens. The occurrence of such bias may relate to the exposure/treatment of interest, therefore it will give rise to the tendency of distorting the study results from the true result. There are also many different selection bias, such as self-selection bias, differential losses to follow-up, Berksonian bias, etc. Information Bias is caused by erroneous measures of the needed information on the exposure/treatment/independent variable, the outcome/dependent variable, or other relevant covariates. Information bias often results in misclassification of the exposure or/and the outcome. Such misclassifications can be differential or nondifferential. Differential misclassification occurs when the chance of being misclassified differs across study groups (exposed/unexposed groups, treated/untreated groups). The direction and magnitude of the bias needs to be further scrutinized. Nondifferential misclassification occurs when the misclassification is independent from other studied variables (exposure, outcome). It usually biases the results towards the null. Confounding Bias is caused by confounding variables, which have independent associations with both the exposure/treatment/dependent variable and the outcome/dependent variable. Confounding leads to a noncausal association that is likely a distorted version of the true (causal) relationship. Usually by adjusting for the confounders can explain away the observed noncausal association. Confounding is a rather complicated concept. Most of the studies will suffer from some sort of confounding. Note: We are only scratching the surface of all the possible biases that we might encounter in our study. To read more about biases, I find this Catalog of Bias from University of Oxford is quite helpful! 1.4.2 How Do Errors Affect Our Study Result? When you show the study results to your advisor or reviewers, what features of your results would they care the most? Most likely, they will ask you a series of questions regarding precision and validity. Precision concerns how close the measurements are to each other. As indicated in the figure above, random errors affect the precision of one measurement. Validity concerns how close the estimate is to the true effect size. Validity includes internal validity and external validity. The latter is also called generalizability. The internal validity evaluates whether the estimate is close to the true effect size of the study subjects, and it is often affected by systematic errors. The external validity concerns whether the estimate is true, such that the estimate from the current study is close to the true effect sizes in other subjects that are not included in the current study. For an excellent discussion of validity in relation to measurement and fairness, please refer to this paper by Jacobs and Wallach. As shown in the following figure, the random error alone usually affects the study precision, systematic error affects the study validity. For example, a noisy measurement of the key exposure variable, as a random error by definition, will result in way less concrete inference of the exposure, in some cases, it can be expressed as wide confidence intervals. Whereas, only selecting healthy participants to test out COVID-19 vaccine, as a systematic error, will potentially produce a false conclusion that the vaccine is effective. However, the direction and magnitude of the bias caused by systematic error must be discussed based on the study specific conditions. When both random error and systematic error occur in one study. The results could be quite questionable… But you don’t want to wait till you see this happens in your study. Therefore, based on what you know about the source of data and your study design, you shall try to anticipate the potential errors that you may encounter. Hence, address such concerns in the design phase to reduce the chance of having such anticipated errors. Figure 1.4: Figure credit: Yan Min 1.5 Typical Biases in Different Data Sources and Study Designs Note: Here, I would like to give a case study as an example of how to incorporate all the things we have talked about into practice. [add case study example] 1.6 How to Reduce and Estimate Different Types of Errors Random Errors. Again, the random error is unpredictable and non-systematic, and it affects the precision of the study estimate. There are several ways to reduce such error: Provide sufficient uniform training to all the personnel taking the measurements. Maintain good experimental techniques. When taking the measurement, plan to take repeated measurements and take the average. E.g, when measuring the blood pressure of patients, measure three times to avoid random variability of the patient’s blood pressure. When measuring the concentration of a chemical solution, also measure more than one time to take into account the intra-variability of the researcher. Increase the sample sizes. As the standard error is inversely correlated to the sample size N (recall “the square-root law”) To effectively estimate the random error, we need to consider both the measurement error and the sampling error. Systematic Error. Systematic error, AKA systematic bias comes in different forms. There is no universal panacea to reduce all the systematic errors. The best way to cope with systematic errors is to anticipate the sources and the causes of these biases in your study based on your study design. Note that it is possible to correct for systematic bias, if you can adequately characterize that bias. For example, in this study, the authors were able to accurately measure public opinion by polling people on the Xbox. Xbox users are, unsurprisingly, a heavily biased subset of the population. Nevertheless, because the authors had information about the respondents, they were able to correct for this distortion. Bias analysis is a very active methodological search field. Most common approach is to conduct different types of sensitivity analysis to evaluate the tolerance of bias in the estimated results. 1.7 Create Your Analytic Plan 1.7.1 Choose Your Weapon 1.7.2 Mind Your Power “Nearly all men can stand adversity, but if you want to test a man’s character, give him power.” Abraham Lincoln Inspired by Lincoln’s quote, I am thinking “nearly all research results can stand a long, baffling discussion section in the researcher’s own paper, but if you want to test the result’s true scientific plausibility, give the research power!” In statistics, power is defined as the probability of avoiding a Type II error. In other words power is the chance that a test of significance will detect an effect conditioned on the effect does exist. In general, power is related to the prespecified significance level, the study sample size, the number of studied factors/interventions/exposures/treatment, and the anticipated effect sizes. This section is to motivate young researchers to think about power calculation, whenever it is applicable to their studies. For more detailed information on power calculation, I find this simple tutorial from Boston University very clear and can be a good starting point. So when do we need to conduct a power calculation? To answer this question, I tend to think about the following criteria: Do I need to collect the data? Does my research question contain a hypothesis for me to test? Is power calculation required because it is part of a grant application? (#fig:power_calculation)Figure credit: Yan Min Power calculation is commonly conducted when the research question contains a hypothesis to test and the researchers need to design an experiment to collect the first-hand data to test the hypothesis. For example, a randomized controlled trial to test the effectiveness of a new drug in treating systemic lupus. With limited amounts of time and resources, researchers often need to determine how many people they should enroll in each arm of the study. Assuming the study has two arms, a new drug arm and an old drug arm. Then they need to decide whether the participants match across arms or not, if so, what is the matching ratio? 1 treated to 2 control? Or 1 treated to 5 controls? They then specify a significance level that conventionally set at 0.05, and an anticipated effect size of alleviating the disease symptoms. Once this is in place, researchers may calculate the sample sizes needed to achieve a reasonably high power. This way, the researchers are more likely to find the sweet point of a reasonable sample size that could achieve a reasonable high power. When researchers do not do any calculation and just wing it, if the sample size is way larger than the actual sample size needed to detect the effect, good! They will probably just spend more money and time to conduct the trial. However, if somehow the sample size is not sufficient at all, then they will be in trouble. After having spent all the money and the time to complete the entire trial, the researchers won’t be able to make any concrete conclusion of the drug being tested in the clinical trial. Hence, if possible, always do your power calculation if it is applicable to your research setting. Although power calculation is not the panacea for under-powered studies, it at least provides reasonable guidance on where the results might be heading prior to actually having the data. Although power analysis is most commonly used in human subjects experiments, it is also relevant to other areas of data science, including user studies and benchmark comparisons. Just as you need a certain number of people in your psychology study to be able to have a decent chance of being able to detect the true effect as significant, you need a sufficient number of raters to be able to test whether people actually prefer your system. Similarly, to provide convincing evidence that your new model is better than the previous state of the art, the test set needs to be big enough to be able to have a good chance of detecting the improvement as significant. For more details on this connection, have a look at &lt;Dallas’ paper that will be on arxiv in two weeks&gt;. Note: More articles are pointing out that 0.05 is fairly a low bar for meaningful scientific discovery. Even Fisher himself, who first brought up this 0.05 as the cutoff point, mentioned in his original writing that this 0.05 only means that the result is worth further exploration, which has nothing to do with confirming a scientific finding. Note: How high the power is high enough? 1.7.3 Need a Pilot? 1.7.3.1 What is a Pilot Study? A pilot study is a small-scaled experiment to test the procedure and methods that could be expanded to a larger scale. It is like a mini replica of your actual study. Depending on the needs of the particular study design, the main goals of the pilot study include: Testing the feasibility recruitment protocol and the informed consent process. This will inform the researchers the feasibility of reaching out to the target population, and anticipate the potential response rate. Researchers will also understand how interested the target population is in the proposed research topic and intervention. Assessing the randomization procedure. If the study calls for randomization, the randomization procedure itself should be tested prior to the actual study. Since a failed randomization process will jeopardize the overall validity of the study. Testing the “flow” of the study protocol. The study protocol often involves multiple steps for the researchers and participants to fulfill. The integrity of this “flow” is crucial. For example, after “filtering” participants using the prespecified inclusion/exclusion criteria, the participants will be invited to a lab for blood draw, and then the blood sample will be prepared and transported, etc. All these steps need to be smoothly connected, so that the participants will feel less hectic, and the sample quality is guaranteed. Evaluating metrics used for data collection. Studies using denovo questionnaires or other forms of data collection tools need to pre-test these approaches in the target population. So that researchers may catch any glitches in the tools before it is too late! Also if there are more than one tools, researchers may use the pilot to compare which tool is the best regarding ease in administering, recording, and analyzing. Gaining insights on the quality and feasibility of gathering data on the study outcomes. Oftentimes, researchers would like to think of multiple possible outcomes to evaluate the effects from the exposure/intervention/treatment. The gold standard version of the outcome could be the best theoretically, however, in practice, it might not be the case. For example, when measuring fat mass in the target population, we can use body mass index (BMI), where you only need to measure weight and height; or use dual x-ray absorptiometry (DXA) to measure body composition, which tells us the volume of fat mass, lean mass, and bone mass; a third option would be using computed tomography (CT), then we can also differentiate visceral fat from subcutaneous fat. However, assuming we do not know which one is the most feasible, both cost wise and time wise, then we shall use the insights gained from the pilot study to guide our decision making. Familiarizing the research team with the study protocol. A pilot study can also be the opportunity for the team members to get familiarized with every single step of the study. This can be viewed as part of team training. What kind of error(s) does this step reduce? Random error or blunders! 1.7.4 So, Do You Need a Pilot? People say you never test the depth of the water with both feet. Hence, my answer to this question would be “Yes, more or less!”. Whenever you have any concerns or questions regarding the execution of the study, a pilot study may just provide the insight that you need. Since its focus is on the execution instead of the actual analytical results and it is small-scale, a pilot study can amplify the “signals” at each glitch of the research execution. However, we need to be mindful of the kind of concerns that a pilot study cannot address: A pilot study cannot be used to evaluate any adverse effects on the studied population. Specific trial design is needed to evaluate toxicity, safety, and tolerability of an exposure/treatment/intervention. It also cannot inform effect sizes used in the sample calculation for the main study. Again the goal of a pilot study is to evaluate the process, not to inform the results. The small sample size for a pilot study usually provides a very unstable estimate of the effect size. Following the second point, a pilot study also cannot be used to inform the results of hypothesis testing. Please do not use pilot study to test multiple alternative hypotheses and choose the significant one to be tested in the main study. First, this increases the chance of getting a Type I error (false positive); secondly, something is significant in a pilot study, might not be significant in the main study anyway. (Conditioned on the null hypothesis being true, the p-values actually follow a uniform distribution.) 1.7.5 Fun Readings and Additional Resources 1.7.5.1 On Pilot Study Design and analysis of pilot studies: recommendations for good practice. Pilot Studies: Common Uses and Misuses. Guidelines for Reporting Non-randomised Pilot and Feasibility Studies Internal Pilots for Observational Studies 1.8 Start Documenting Your Grand Study “Documentation is a love letter that you write to your future self.” Damian Conway Okay, let’s be honest. Documentation is way less exciting than the previous three parts, but it is a crucial part of all research projects. This Chapter will only serve a motivational purpose on this topic, the following two sections of this manual will discuss this topic in great detail. Simply put, it makes your life easier. Well-documented studies are more likely to be reproduced, first, by yourself; then researchers from other groups. Imagine your reviewers ask you the following questions: How the data has been quality-controlled? Which participants are excluded in the final analysis? And why? Why did you choose 387 as your sample size? Could you please change the color of the figure to make them color-blind friendly? Before running the blood assays, how were the blood samples prepared? Why in the questionnaire you design, question A is after question B? What are your ordering test results? These are the kinds of questions that keep me awake at night. To provide the answers, we usually need to go back into the nitty gritty details of the research conduct. What if we did not document all these, should we just count on our vague memory? Trust me, although we are still young, for a majority of us, our memories are not that trustworthy. Of course the motivation of well documenting our research projects should not be to answer questions from the reviewers. What really matters is that we, as primary researchers, know what we have done to your study step-by-step in great details. This is a determination of moving away from sloppy, hand-wavy science! This is always a way to show our love to our future selves by reducing the ambiguity of what has actually been done already. As mentioned earlier, another purpose of documentation is to serve a greater research community and, together, to conduct high quality scientific research. Therefore, the field is populated by high quality, reproducible advancements, instead of spotty significant results here and there. To serve these two purposes, the documentation plan can be divided into internal documentation and external documentation. The former is more self-serving and mostly done on an internal platform, the latter is serving a greater deed in the research community and the scientific field and mostly done on an external platform. 1.8.1 Internal Documentation Sampling and recruiting protocols and any justified modifications. Experiment/wet lab routines, procedures, and personnel. Protocol and results of the pilot study. Development of any instruments/metrics/questionnaires used in the project, and - their validation procedures. Device calibration process. Data quality control protocols, variable definitions, any kind of data transformations applied prior to the analysis. All the analysis that has been performed (not just the one you “like” the most). Codes (or software settings) used to generate figures and tables. "],["analysis-phase.html", "2 Analysis phase 2.1 Start here 2.2 Statistical Analysis Plan (SAP) 2.3 Data generation 2.4 Data preparation 2.5 Data visualization 2.6 Data summarization 2.7 Data analysis 2.8 Data analysis - medicine 2.9 Statistical analysis report 2.10 Examples 2.11 Resources", " 2 Analysis phase This portion of the manual offers practical guidance and warns against common pitfalls that may be encountered from the point of data generation to the point of data analysis. This collection does not aim to be exhaustive, but rather offer a collection of major pain points that we have encountered in the process of scientific research and an account of how we, and many others, have gone about solving them. The scientific field is enormous and ever changing, for which reason we do not expect our guidance to perfectly apply across all fields, or any field for that matter. However, we do hope that, even if much of this does not apply to you, that it will inspire you and give you a sense of how to go about practicing transparent, rigorous and reproducible science within your own field in your own way. 2.1 Start here “If you have built castles in the air, your work need not be lost; that is where they should be. Now put the foundations under them.” Henry David Thoreau Read this manual BEFORE you begin your study. You need to know what data you should be gathering and what analyses you should be reporting before you even begin your study. This is because there are analyses that you should produce before you even begin (e.g. power analysis) and data that you should be gathering from the get go (e.g. confounding variables used by other teams) that you may not even think of gathering in advance. Recognize and avoid cognitive biases. It is important to recognize that we are all subject to cognitive biases and take steps to mitigate them before you even begin. Confirmation bias is the tendency to generate, analyze and interpret data designed to confirm rather than challenge your hypotheses. Examples of this in science may include running experiments that you know are more likely to confirm rather than dispute your hypotheses, or avoiding tests that may cast a doubt on the robustness of your findings. Try out this fun problem by New York Times for a demonstration of confirmation bias. Overconfidence bias is the tendency to place more confidence in your judgment or ability than is due. Examples of this in science may include attribution of failed replications to someone else’s inferior skill or attribution of unexpectedly positive results to your superior skill. Try out this fun test of your ability to assess subjective probability (overconfidence bias arises from a miscalibration of subjective probability). There are many other relevant biases, such as availability bias, anchoring bias or affinity bias. Think what steps you can take to mitigate the impact of these biases before you begin. Some of the strategies detailed in this chapter (e.g. creating a protocol) represent our approach to do just that. Pro tip: We often find ourselves being overconfident in our analyses and not running that extra “pointless” check or test. We must say that we have never, ever, ran that extra test without obtaining insightful results that more often than not challenge the confidence we had initially placed in our analyses. 2.2 Statistical Analysis Plan (SAP) “To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.” Ronald Fisher Collaborate with a statistician. Yes, the time to speak with a statistician is BEFORE you even begin your study, not after! You can request free statistical consulting at Stanford here, no matter what research you do. By collaborating with a statistician, you are not only improving the quality of your analyses, but you are also learning from the experts. Collaborating with a statistician is a strength, not a weakness. Find the most appropriate reporting guidelines. These phenomenally useful guidelines indicate how authors should go about reporting their study appropriately. Different disciplines maintain different guidelines for different study designs, most of which can be found at the EQUATOR Network. For example, guidelines exist on how to report randomized clinical trials, observational studies, study protocols, animal pre-clinical studies, lab-based experimental studies, machine learning predictive models, and many more. Pro tip: There may not be a reporting guideline that fits your needs. We find it helpful to just go through relevant guidelines, extract what elements apply to our studies and use them as a quasi-guideline. Convert the identified guidelines into your first draft. Take the appropriate reporting guidelines and turn them into subheadings. Then, fill in the blanks to the best of your ability. Do not start your study unless you have completed everything that you could possibly complete a priori. Congratulations, you did not only just create a rudimentary protocol, but in fact you just created your first draft! Yes, the first draft arrived before you even began your study, at zero extra effort. And yes, you write and publish a paper, whether the results support your hypotheses or not. Pro tip: As soon as you publish a couple of times using your preferred reporting guidelines, you can save time by simply using previous papers as the foundation for your next paper. Pro tip: Do not worry if your methods are too long. This is the 21st century, not the 1950s - should your eventual paper be too long, simply shift the more detailed version of your work to the supplement (see more details about this below). Having said that, there is much merit in being concise and little merit in dumping an enormous amount of barely meaningful information in the supplement. Cite the guidelines you have chosen. Cite the reporting guidelines that you are using (e.g. at the very top of your methods section). By doing so, you are spreading the word, acknowledging those that spent time producing these guidelines, boosting the perceived reliability of your own study and helping meta-researchers interested in studying the use of such guidelines. Clarify a priori versus post hoc decisions. As mentioned in the first part of this manual, it is extremely important to distinguish between confirmatory (a priori) versus exploratory (post hoc) analyses. The former are independent of the data at hand, whereas the latter are dependent and decided upon after the data at hand were seen. Often in today’s literature, exploratory studies masquerade as confirmatory studies, because of incentives, such as being published in more prestigious journals. This is often referred to as HARKing (Hypothesizing After the Results are Known) and it refers to studies where an exploratory analysis led to a specific hypothesis, which is then studied as if the authors intended to study this hypothesis all along. This is problematic because the hypothesis is highly dependent on the particular sample (had we used a different sample this hypothesis may not have been deemed relevant) and it conveys a misleading level of confidence to readers. The beauty of having already created a protocol as your first draft is that you have already clarified which analyses were decided upon a priori - you can then go back and add any future data-dependent analyses to this draft by indicating that the following decisions were taken post hoc. Pro tip: There is NOTHING wrong with clearly reporting that your study is an exploratory analysis - in fact, doing so would afford a freedom and peace of mind to approach your data however you want, rather than being stranded in the no man’s land of trying to use your exploratory analysis but also constrain yourself to comply with the requirements of a confirmatory analysis, doing neither of the two well in the end. Create a Statistical Analysis Plan (SAP). Even though not always necessary or universally recommended, you can take your aforementioned protocol up a notch by creating a SAP (all before you collect your data). This is a more detailed account of your statistical approach, even though a well-created protocol should have already mentioned most of what a SAP may include. Two excellent guides on how to construct a SAP can be found here and here. Pro tip: Creating a SAP may look daunting at first, but you really only have to work through the process once. After that, as mentioned above, you can simply model future SAPs onto what you did before. Pro tip: As indicated above, should you decide to create a more detailed SAP, do not deviate from the format of your eventual paper - you do not want to be duplicating any work. If by being rigorous and transparent you are spending more time on a project than you did before, you are probably doing something wrong. Register your protocol/SAP. Optionally, you can submit your protocol for registration at repositories, such as ClinicalTrials.gov and Open Science Framework (OSF). Registration will: (a) hold you accountable to your a priori decisions (we are all human, the drive to change a priori decisions can be substantial and having the registration anchor can be a useful cognitive lifeboat), (b) inform researchers working in the same field about studies underway (if afraid of scooping, you can often request an embargo period) and (c) assist meta-researchers interested in studying how many studies are done and which of those studies are published. Publish your protocol. Optionally, you can do one of two things: (a) you can have your paper accepted on the basis of its protocol alone and before the results are known (this is known as a registered report) or (b) publish it as a standalone protocol, if the protocol itself represents substantial work (this is often done in medicine for large clinical trials or prospective cohort studies). Pro tip: We always begin our write up before we even start working on a study. We take the appropriate reporting guideline, convert it into subheadings and then fill in the blanks. We fill in whatever is not data-dependent before the study (which is the majority of subheadings) and whatever is data-dependent after the study. Any data-dependent changes we clearly indicate as “post hoc.” 2.3 Data generation “No amount of experimentation can prove me right; a single experiment can prove me wrong.” Albert Einstein If possible, run a pilot. It is very common to realize shortcomings in your protocol as soon as you start gathering data. A small pilot study is often a great way to avoid changing your protocol after study initiation and pretending that this was the protocol all along. If you do absolutely need to change your protocol, indicate what changes were made post hoc because such decisions are likely data-dependent. Again, there is nothing wrong in being clear about post hoc decisions and nothing wrong about reporting failed experiments; however, much is wrong with not reporting data-dependent changes or initial failed experiments and these may even amount to scientific misconduct. Pro tip: Pilots can be very helpful in imaging studies, where saturated pixels may take you outside of your instrument’s linear range and complicate your analyses - use pilot studies to identify the desired antibody dilutions. DO NOT pick and choose. It is often convenient, and very tempting, to pick and choose data points that promote your point of view, but do not necessarily accurately reflect the whole. For example, in microscopy you may image a field of view that clearly illustrates the effect of interest to you, whether or not this is a representative sample of the whole picture. Similarly, in clinical trials you might not enroll patients that you think are less likely to respond, or overzealously enroll patients that you think are likely to respond. This practice may wrongly inflate the effect you are trying to quantify and wrongly deflate the observed uncertainty. In clinical trials this is mitigated by blinding (the doctor and patient do not know whether they are on treatment or placebo) and allocation concealment (the doctor does not know whether they are about to allocate the patient to treatment or placebo). In microscopy, this can be mitigated by only imaging at a number of predefined XY coordinates. Feel free to come up with methods that suit your needs, using the guiding principles of being systematic, honest and not fooling yourself or others. There is nothing magical about repeats of three. This is particularly relevant to biologists, who seem to like repeating experiments three times, or even four times if they want to feel extra rigorous (or, if the first three did not reach statistical significance, God forbid). First, always decide how many times you would like to repeat BEFORE you begin your experiments. If you have already begun, then it is too late - deciding how many times to repeat an experiment on the basis of the p-value is extremely problematic. If you cannot resist the temptation of chasing the p-value, (a) clearly mention in your manuscript that you decided to add an extra repeat to achieve significance and (b) speak with a statistician to help you analyze your data (it is possible to account for chasing the p-value in your statistical analysis and still arrive to a correct p-value). Second, use a power analysis to decide how many repeats you need. The rule of thumb in clinical research is to go for 80% power (i.e. have an 80% probability of a sufficiently low p-value when the null hypothesis is false). If you do not know how to do a power analysis, (a) speak with a statistician, (b) most statistical packages can do this for you (see here for GraphPad Prism; see here for R). Pro tip: Yes, doing a power analysis for the first time will be time-consuming and will feel daunting and pointless. However, after a few times, you will become a pro. Pre-determine your sample size or repetitions. Known as stopping time, this is the time you stop data generation. This time may be the number of participants you decide to recruit, the number of experiment repetitions you decide to run or the number of simulations you decide to run. Not abiding to such a stopping rule is sometimes referred to as sampling to a foregone conclusion and it is a bulletproof method to get as small a p-value as you like. However, this p-value is at best meaningless and at worst deceitful. As indicated above, always decide your stopping time BEFORE you begin your study. If you do want to proceed until you get a low enough p-value, this is possible, but you will need to analyze your data differently - if you do not know how to do that, speak with a statistician. Use a specific seed when simulating data. If your data generation involves data simulation, always use a seed (e.g. in R do “set.seed(123)”), otherwise your procedure will not be reproducible (because every time you run the code you will get a different set of random numbers). Note that random numbers may appear in algorithms that you did not even expect. Also note that if you are using parallel processing you will need to set the seed in each of the cores you are using, which is not always straightforward and may require a bit of digging to find out how to do it if you have never done it. Testing your code on multiple systems and verifying that you can reproduce the same results (even on simulated data), is a good way to help ensure that you have set the seed correctly. Update your draft. It is almost impossible to have generated your data without encountering something that you did not foresee. If you did, now is the time to mention this in your draft and make any other necessary changes. Clearly indicate that these changes were made after observing the data. 2.4 Data preparation Create an analytic dataset. Keep the data obtained from your data generation process as your “raw dataset.” Import your raw dataset into your program of preference to prepare your data for analysis - this new dataset will be your “analytic dataset.” This involves steps such as data validation (make sure that all of your data make sense, e.g. nobody is more than 150 years old!), data standardization (e.g. someone may have reported they are from the USA whereas someone else may have reported they are from the United States - these two need to be standardized according to your preferences) and dealing with missing data (see below). NEVER MODIFY YOUR RAW DATA, however easy it may be to reobtain. Keep the code required to create the analytic dataset separate from the code used for data analysis - this modular approach to data creation leads to much tidier code and allows you to easily recreate your analytic dataset whenever you realize that something was missing. Pro tip: Name your analytic dataset after your code file - this way anyone can easily tell which file created each analytic dataset. Pro tip: Have a look at the Publication phase for more details on how to organize your files on your computer appropriately. The DOs of analytic datasets. Things you should always do: Make sure that there is a column or field that uniquely identifies each observation. This column is often called an “ID” column. Duplicates almost always creep in. Make sure that each column has a unique name, i.e. no two columns have the same name. This is because some programming languages cannot handle datasets with non-unique column names and we want our dataset to be easily used by everyone no matter software. Make sure that column names do not include any spaces, line breaks or punctuation other than “_”. This is for the same reason as above. Give expressive but concise names to columns. If in doubt, always go for “too long” a name than “too short”. Ideally, even though not always possible, others should be able to tell what the column means just by reading the column name and without reading the data dictionary. Make sure that you save your analytic dataset in a format that anyone can access (e.g. CSV, JSON, XML). Do not save as .xlsx because not everyone uses Microsoft Excel. The DONTs of analytic datasets. Things you should never do: Never remove outliers at this stage. If you do want to remove outliers, which is rarely recommended, only do this while analyzing the data. Never convert continuous variables into categorical variables (e.g. do not replace ages above 80 with ≥80 years old and ages below 80 with &lt;80 years old) because you are losing information. This is an analytic choice that should be taken into account later on while analyzing your data. Never edit data by hand in Excel as it is impossible for others to replicate this process (if you really want to do so, even though you should not, create a detailed guide of what steps someone should follow to convert the raw data into your analytic data in Excel). Also be careful when it comes to dates and times, as these are sometimes handled differently by different systems. As a safety measure, it may be worth saving the essential components of the data in separate columns (e.g., adding additional columns for year, month, and day, if that is the level of granularity at which the data were acquired). Pro tip: A good convention is to keep column names lower-case and separate words by “_” (e.g. medical_history). You can also use the following prefixes to name columns in ways that reflect their data type: (a) “is_” denotes a binary variable with entries as Yes/No or TRUE/FALSE (e.g. “is_old” could denote patients older than 80 years), (b) “num_” denotes a continuous variable (e.g. “num_age” could denote the age) and “n_” denotes counts (e.g. “n_years” could denote number of years). Finally, consider using the unit of measurement within the column name when this is ambiguous; for example, “years_since_onset” is preferable to “time_since_onset” because the former clarifies that the column is in “years” and obviates confusion or the need to check the data dictionary. Pro tip: The better the column names, the less often a reader should need to consult the data dictionary. Even though often inevitable, we consider it a failure when someone cannot deduce the contents of a column simply by reading the study and the name of the column. Dealing with missing data. This is complicated, there is no satisfying answer and many pages have been written about it (probably because there is no satisfying answer). However, there is a very clear answer to what you shouldn’t do, and this is that you should NEVER pretend that there was no missingness! Whether you decide to do something about the missingness or not (e.g. imputation, sensitivity analyses, etc.), ALWAYS indicate how many observations from what variables were missing. In terms of what to do about it, a rough rule of thumb is to leave it as is if &lt; 5% of observations are affected, otherwise use multiple imputation: Figure 1 of this guide is a great summary. Pro tip: If you decide to impute, always report results from the imputed and the original dataset. Again, there may be a problem in reporting too little, there is rarely a problem with reporting too much. NEVER throw away data. Do not crop away data from images or Western blots. Do not get rid of observations with inconvenient measurements. These are analytic choices that should be taken during data analysis - see further details in Data analysis. Update your draft. It is almost impossible to have not encountered something that you did not foresee. If you did, now is the time to mention this in your draft and make any other necessary changes. Clearly indicate that these changes were made after observing the data. 2.5 Data visualization “The greatest value of a picture is when it forces us to notice what we never expected to see.” John Tukey Always visualize your data. Countless errors have been caught by this simple step, to visualize your data using a simple histogram, box plot and scatter plot (the figure below is known as Anscombe’s quartet and shows four scatter plots of very different distributions but identical correlation). In R, you can do this for the whole dataset using a single command (“ggally::ggpairs” - note that if using this command, you may want to prioritize visualizing the top 10-15 most interesting variables for clarity). You can upload these visualizations as a supplement to offer a feel of your data to readers with no access to your data. (#fig:anscombe_quartet)Figure credit: https://en.wikipedia.org/wiki/Anscombe%27s_quartet If you are plotting a bar chart, you are probably doing something wrong. It is a very common practice in biology to take a few points, such as 3 means each from a different experiment, and plot a bar chart of their mean. This is very misleading because it conceals the uncertainty in each of the means. So what should you do? If you have summary statistics (e.g. means) from repeat experiments, plot them with standard error bars - this is nicely done in R by using dot-whisker plots. If you have many data points (e.g. age of participants), DO NOT create a bar chart of their mean or any other statistic - create a boxplot. Never use a pie chart. Thankfully, this was answered for us by the father of data science himself, John Tukey: “There is no data that can be displayed in a pie chart, that cannot be displayed BETTER in some other type of chart.” Plot the individual data. Try to, as far as possible, plot all individual data points. For example, if you are plotting a boxplot, also include the individual data points (e.g. by using ggplot2:geom_jitter or ggplot2::geom_dotplot). If you are plotting a regression line, do not just plot the line of best fit, also include a scatterplot of your individual data. Pro tip: Remember what we said earlier about random numbers creeping up on you? If using geom_jitter in R (which you should), you need to use a seed to create a reproducible plot. To do so, define the position argument of the command like so: geom_jitter(position = position_jitter(seed = 123)). Update your draft. You now have your first results. Use the reporting guideline you had chosen above for guidance on how to report your results. Be concise, but at the same time transparent, comprehensive and honest. By writing all of your thoughts while analyzing your data and presenting your analyses (e.g. in lab meetings) you are maximizing your efficiency as you will not have to repeat the same work when you are done. Remember to clearly indicate any changes to your initial plan and clearly indicate that these were made after observing the data. 2.6 Data summarization “The first principle is not to fool yourself, and you are the easiest person to fool.” Richard Feynman Produce summary statistics for all of your data. Always summarize your data for the reader (e.g. produce means, standard deviation, medians, interquartile ranges, etc.). This is standard in some fields (e.g. medicine), but not others (e.g. computer science). Even in fields in which it is standard, most will only produce summary statistics of just a few variables. However, in R, with a single line of code you can summarize all of your data using the packages skimr or summarytools (preferred) - both even produce a histogram for each variable as well. You can then submit the output of the summarytools package as a separate file to be published alongside your article. We cannot stress enough that nowadays, using computers, ANY form of data output can be made available and we should apply no restrictions, given how easy it is to produce such summaries. You should probably be using the median, not the mean. In summarizing skewed variables, the median and interquartile range are usually more meaningful than the mean and standard deviation. However, again, you should not feel that you need to prioritize and should you want, you can report both (e.g. report the median in the main table of your text and report the median and mean in the automatically created summary tables included in your supplement). Update your draft. As above, no need to wait until the end, be efficient and clearly indicate any post hoc decisions as such. 2.7 Data analysis “All models are wrong, but some are useful.” George Box Decide whether you plan to describe, predict or estimate. This is arguably the most important data analytic decision you need to make, it should be made at the outset and it should be clearly communicated. Miguel Hernan and colleagues recently proposed that most analytic tasks can be classified into description, prediction or counterfactual prediction (or, estimation). Description is “using data to provide a quantitative summary of certain features of the world. Descriptive tasks include, for example, computing the proportion of individuals with diabetes in a large healthcare database and representing social networks in a community.” “Prediction is using data to map some features of the world (the inputs) to other features of the world (the outputs). Prediction often starts with simple tasks (quantifying the association between albumin levels at admission and death within one week among patients in the intensive care unit) and then progresses to more complex ones (using hundreds of variables measured at admission to predict which patients are more likely to die within one week).” “Counterfactual prediction is using data to predict certain features of the world as if the world had been different, which is required in causal inference applications. An example of causal inference is the estimation of the mortality rate that would have been observed if all individuals in a study population had received screening for colorectal cancer vs. if they had not received screening.” This distinction is very important because these three tasks require fundamentally different approaches, even if you are using the same statistical machinery. For example, when using linear regression, in description you would be interested in fitting to the data at hand, in prediction you are interested in fitting to future unobserved data and in counterfactual prediction you are estimated in fitting to an unobserved world. In biology, most labs are interested in counterfactual prediction (where the control is the counterfactual). In medicine, most teams are also interested in counterfactual prediction, but in fact are publishing descriptions or predictions. Test and report ALL possible analytic choices, not just your favorite. Named as “The Garden of Forking Paths” by the famous statistician Andrew Gelman and often referred to as “researcher degrees of freedom”, this is the practice of reporting the results of a very specific set of analytic choices, where many other legitimate choices exist and where, had the data been different, different choices would have been made. A fantastic study by Botvinik-Nezer et al. (2020), assigned identical data to 70 independent teams and tasked them to come up with p-values for 9 different hypotheses - they identified high levels of disagreement across teams for most hypotheses and that for every hypothesis there were at least four legitimate analytic pipelines that could produce a sufficiently low p-value. Gelman suggests that these problems could be solved by: (a) pre-registering intended analyses before any data is obtained and (b) following up the results of a non-preregistered exploratory analysis by a pre-registered confirmatory analysis. Other approaches are: (a) define all possible analytic choices and have them all run automatically, (b) have the same data analyzed by a few separate teams (this is widely practiced in genomics) or (c) define all analytic choices and the conditions that would lead to each choice and then bootstrap the whole analytic path, not just the final analysis. For an example of the last approach, say out of 1000 observations you identify a few outliers that you would like to remove - do not simply remove those points and rerun your analyses; instead, run a bootstrap where points beyond a predetermined threshold are removed - this is the correct p-value that should be reported. Pro tip: All approaches to solving the problem of forking paths are excellent, but the simplest is to just report everything you did. If you fitted multiple models and one of those works best, do not just report the one model, report all models, compare and contrast them. If you are deciding between fitting a few models, fit all of them, compare and contrast them. Incorporate analyses done by others, do not just analyze your data your way. Even though it is common practice in some disciplines to incorporate analyses from previous papers, such as comparing the performance of your new machine learning method with previous methods on the same data, in other disciplines it is not. In fact, in medicine, different groups tend to analyze different data differently, such that no two groups ever create similar results and no two studies can be reliably combined into useful knowledge despite working on the same question for more than half a century (Serghiou et al., 2016). For example, if a previous study analyzed their data using propensity matching and you want to use an instrumental variable in your data, do both. If a previous group included smoking as a confounder in their model, but you do not think this is a confounder, fit two models, one with smoking, one without. If you are developing a new prediction model, compare its performance to prediction models developed previously within the same data. If you are trying to extend the work of another lab, which often implies that you need to develop their own approach and replicate some of their experiments, do not just report the new experiments, report the whole process - this will help us understand how effect sizes vary across labs and in combination with estimates from previous labs we will have a better estimate of the true underlying effect. Pro tip: Wet labs often do not produce exact replications of previous work. They deem those rather boring and time-consuming, for which reason they usually produce conceptual replications, i.e. use different often less involved methods to test the same hypothesis. This is often problematic because it is impossible to interpret the findings of these experiments without reproducing the previous experiments. Did your findings support previous findings for the same reason? Did your findings not support the previous findings because the published experiments were in fact an artefact or for some other reason? These problems mean that these half-hearted replication approaches often end up on the shelf, never published and as such in the end take up more time than the exact replications. As such, try to engage in exact replications, do those well and with a registered plan and publish your results - many journals, such as eLife, accept replications. Control for multiple testing. You may be surprised to know that out of 10 clinical trials of ineffective drugs, the chance of getting at least one p-value &lt; 0.05 is 40%! So what should you do if your work involves calculating multiple p-values? First, as a rule of thumb, if you are running more than ~10 statistical tests, you need to use a procedure to control the false discovery rate (i.e. the probability of obtaining false positive p-values). We recommend using the Benjamini-Yekutieli procedure, which is better than the Bonferroni correction (assumes independence) and cleverer than the Benjamini-Hochberg. You do not need to know much about this, other than most statistical programs can do it automatically for you (see here for R; see here for GraphPad Prism). Second, if you are comparing between more than two conditions (e.g. outcome with drug A, B, C or D), first test for the presence of a difference between at least two conditions (e.g. using the one-way ANOVA to test for a difference in means). A positive test suggests that there is a difference between at least two conditions. If a difference is found and you would like to know where the differences lie, you need to use a post hoc test. The most commonly used post hoc test for one-way ANOVA is Tukey’s honest significant difference method (see here for R; see here for GraphPad Prism) - using a specialized post hoc procedure is important to maintain the desired false discovery rate. Pro tip: It can often be tricky to know whether you should control for the FDR or not. Even though this sounds like you should be talking with a statistician, briefly, we use two rules of thumb: (1) count the number of p-values that could constitute a “finding” within each one of your objectives (e.g. if your objective is to find which one of 20 characteristics may cause cancer, then a low p-value in either one of those 20 would constitute a “finding”, so your count is 20); (2) individually control the FDR for any objective where &gt;10 p-values could constitute a finding. Compare effect sizes, not p-values. Comparisons of p-values are meaningless. For example, you cannot say that there was a statistically significant improvement in the group taking the new drug (e.g. p-value = 0.003), no statistically significant improvement in the placebo group (e.g. p-value = 0.3), hence the drug is effective! You have to statistically test the change in the new drug group against the change in the placebo group. Pro tip: This is a fantastically common problem. Articles often do not actually test the hypothesis of interest. Instead, they test many other hypotheses and then verbally combine the results of those hypotheses to claim significance in the hypothesis of interest. Never combine means - or any other statistic - as if they were data points. It is very common practice in biology to take means from separate experiments (e.g. prevalence of a marker in different Petri dishes) and then taking the mean of means as if they were data points. Some may frown upon reading this and suggest that no, you should add the raw data from all experiments and take the mean of that. The bad news is that, neither of the two is correct. The first method falsely disregards the uncertainty in each estimate of the mean and the second approach falsely disregards the uncertainty within and between experimental conditions. So what should we do? Use a meta-analysis: this is far easier than it sounds and the gist of it is to take the weighted mean of your means, where each mean is weighted by its variance. This is known as the Mantel-Haenszel method - you can read more about it and implement it in R with one line of code as per here. Pro tip: We are so frustrated by this problem that we vouch to pay $50 to the first biologist that sends us an email of a manuscript they have just published where all statistics were combined using a meta-analysis. Be cautious when converting continuous data into categorical data. Say you have patients of ages from 25-80 years old and those greater than 65 years old constitute the oldest 10% of your data. A possible analytic decision is to consider everyone &gt;65 years as old and every below as not old. This is fine, BUT, you need to include this analytic decision within your calculation of uncertainty. As we argued above, if you had taken a different sample, the top 10% oldest could have been 50 or 70 years old. You can do so by discretizing your data (i.e. turning them into categories) automatically WITHIN each bootstrap, not before. Set a seed. If your analysis involves any random components (e.g. in a simulation, hyperparameter tuning, parameter initialization, etc.) then make sure that you set the seed to make your work reproducible. For more, see a similar point made in “Data generation”. Collaborate with a statistician. As indicated above, there is no need to do your own statistical analyses and, in fact, there are a lot of upsides to requesting external help. You can request free statistical consulting at Stanford from here. By collaborating with a statistician, you are not only improving the quality of your analyses, but you are also learning. Update your draft. As above, no need to wait until the end, be efficient and clearly indicate any post hoc decisions as such. 2.8 Data analysis - medicine If you can avoid reporting odds ratios (and most likely you can), avoid it. Many report odds ratios when they do a logistic regression. However, very few realize that odds ratios inflate the perceived result (i.e. the odds ratio is always more extreme than the risk ratio, unless it is 1, in which case they are identical) and can change in an exponential fashion. In fact, many are not aware of the difference between odds and risk (i.e. chance) and commonly interpret odds as if they are risks. So what should you do? Convert it to a risk ratio! An example can be found here. If you can report the number needed to treat (and most likely you can), report it. In clinical medical research, it is very common to report odds ratios or risk ratios, but very rare to report the number needed to treat (NNT). The problem is that, in fact, the most relevant metric of them all, in terms of policy making, is the NNT. Always make this number available. 2.9 Statistical analysis report “We have the duty of formulating, of summarizing, and of communicating our conclusion, in intelligible form, in recognition of the right of other free minds to utilize them in making their own decisions.” Ronald Fisher Report EVERYTHING - leave no man behind! Good reporting is not only about how to report, but also what you choose to report. And the answer to what you should report is always EVERYTHING! Up to say 30 years ago, it could make sense to select what you choose to report because scientific communication occurred predominantly via limited printed text. This is not necessary anymore. You can include Supplementary Information of as many pages as you want, submit to journals with no word limits (e.g. PLOS Biology) or submit to journals that do not prioritize novel findings (e.g. eLife, PLOS One). You can even avoid the peer-reviewed literature altogether and submit as a preprint (e.g. bioRxiv) or simply as a Medium article. If an experiment is not worth reporting it should not have been done in the first place - if it was worth even contemplating doing, it is worth being reported. Not reporting it is either a form of data dredging, misuse of taxpayer money, or both. Imagine the poor PhD student spending months of their time and taxpayer money working on an experiment that you could have already told them is a dead end. Pro tip: We have never regretted reporting too much, but we have regretted having reported too little. Pro tip: As a peer reviewer, we have been asked to review articles where authors did report everything, but their method of reporting everything was to simply dump all of their analyses into a massive supplementary file of hundreds of pages. This too is bad practice, as being open is not always equivalent to being transparent. If the information you are providing is difficult to get through that can be a barrier to transparency. Consult the Basu et al. (2020) paper from the Examples to see how they seamlessly included all analyses into their article. Pro tip: If you are in biology, please stop making your best picture available in your papers. Instead, show a picture where the effect is very clear and contrast it with a picture in which the effect is very unclear. Additionally, make ALL pictures available on an online platform (see Data sharing for more details). As above, we pledge $50 to the first person to send us a newly published paper in which they have abided by these principles. Never choose what to report on the basis of the p-value you p-hacker. Known as data dredging, data fishing, data snooping or p-hacking, this is the practice of engineering more confidence in the reported p-value than it is due. Say for example you do an experiment 20 times, you get 19 p-values above 0.05 and one below 0.05. Simply reporting the one “successful” experiment and not reporting the 19 “unsuccessful” experiments is a form of p-hacking because you are falsely inflating the perceived chance that your result is a true positive - had we known that this was only 1/20 experiments, we would likely consider this is a false discovery (i.e. a false positive). Here is a fantastic article by FiveThirtyEight should you want to test your ability in engineering a low p-value. The p-value should never determine whether a result is reported or not. And here is a website to caution you against spurious associations - just because they are correlated, it does not mean that they are causally related. Pro tip: We avoid p-hacking by using a trick suggested by Tukey. We always develop our analytic plan using a massively subsampled version of our data, say using 100 out of 5000 observations. By subsampling our data to the extent that getting a low p-value is almost impossible, we avoid making decisions on the basis of the p-value. When all of our analyses are done, we then switch to using all of our observations, to obtain the results that we will report in the study. Another version of this is known as “honest estimation”: you split your sample in two, develop your model in the first half (i.e. decide what variables to include) and fit it in the second half - to avoid throwing away data, you can do it again in the opposite direction (develop in the second half and fit in the first half) and take the mean of the two models. Figure 2.1: Figure credit: https://en.wikipedia.org/wiki/Anscombe%27s_quartet Report the effect sizes, do not just report p-values. For some reason, we have grown to love the p-values, and grown to disregard the actual effect size. For example, most biologists would be far more interested in whether a correlation is “statistically significant” vs. what the magnitude of that correlation actually was. This is very sad because a correlation coefficient of 0.9 has an enormously different interpretation that a correlation coefficient of 0.1 - in fact, given enough data, any correlation coefficient could be found to be statistically significant. If you were to choose between values to be reported, a rough heuristic would be to first choose the 95% Confidence Interval, then choose the point estimate (e.g. the correlation coefficient) and then choose the p-value. Some researchers have even gone so far as to suggest that p-values should never be reported (Amrhein et al. 2019), which received an equal and opposite response (Ioannidis 2019) - regardless, we are afraid that neither of the two expressed any fondness for the over-reliance on p-values and the under-reporting of effect sizes. Analyze and report your analyses as if you are contributing evidence towards a research question, not necessarily to conclusively answer it. Your study is not meant to conclusively answer a research question. Most research questions are so broad, results vary so much across different conditions, different populations, different species, different cells, etc., and time and money are so limited that you cannot conclusively answer a research question. Science advances by assessing the totality of evidence arising from several studies, which by synthesis and triangulation can be used to come up with the most likely answer. The reason this is important to realize is that you should view your analyses, experimentation and reporting as primarily offering evidence, that in combination with evidence from other labs can be used to estimate the truth. Do not feel the urge to present the one and only best of the best analysis, because it does not exist. Fight the urge to hide those null analyses that you think may weaken your conclusions. Reveal all of your analyses, all of your experiments and incorporate any uncertainty in your findings. Publish or perish. Add the Grimes 2018 paper. 2.10 Examples Basu S, Berkowitz SA, Phillips RL, Bitton A, Landon BE, Phillips RS. Association of Primary Care Physician Supply With Population Mortality in the United States, 2005-2015. JAMA Intern Med. 2019;179(4):506–514. doi:10.1001/jamainternmed.2018.7624 This is a fantastic example in which all analyses and all findings were reported, not just the one and only that the authors believed was the best. Readers have the right to decide for themselves which analysis to believe in and differences between analyses reveal details about the data that would have otherwise been missed. Goldstone AB, Chiu P, Baiocchi M, et al. Mechanical or Biologic Prostheses for Aortic-Valve and Mitral-Valve Replacement. New Engl J Med. 2017;377(19):1847-1857. doi:10.1056/nejmoa1613792 This is a fantastic example where the authors used multiple methods to analyze their data and reported all of their analyses. The authors also went at great length in their supplementary materials to explain their analytic approach in detail, often creating accounts of methods that exceed in quality ones you would find in certain statistical textbooks! Matthew Stephens, False discovery rates: a new deal, Biostatistics, Volume 18, Issue 2, April 2017, Pages 275–294. doi: 10.1093/biostatistics/kxw041 This study was available on GitHub from the get-go, such that you could actually see in real time how the project was changing and what new analyses were being added. The GitHub now serves as a great example of how to organize your projects and can be accessed here. 2.11 Resources 2.11.1 General Best of data science. This is a GitHub repository that one of us (S.S.) maintain and to which he adds his favorite resources related to data science, from books, to packages, to tutorials. The most useful and most actively maintained list is that of R material and you are bound to find most of what you need in order to practice data science in R like a pro (books on R and statistics, guides on data visualization, data summarization, Bayesian statistics, etc.) - if there’s something that you do not find, please raise an issue on the GitHub page! Note that if you are paying money to learn how to code in R and how to practice basic data science you are probably doing something wrong. MiB Open Science support. A great resource with details about various aspects of the transparent, rigorous and reproducible workflow. It contains a section on creating a statistical analysis plan (even though this may be going over the top). Researcher Academy. An effort by ELSEVIER to provide free e-learning modules on research preparation, writing for research, the publication process, navigating peer review and communicating your research. Overall, even though it includes useful information, it certainly falls short in describing how to practice more transparent, rigorous and reproducible research. 2.11.2 Statistical analysis plan EQUATOR Network. A comprehensive searchable database of reporting guidelines and relevant resources. At the time of writing, EQUATOR maintained an index of 432 reporting guidelines! Principles and Guidelines for Reporting Pre-clinical Research by the NIH. A consensus guideline from editors of 30 basic/preclinical science journals. Even though not as explicit as the ARRIVE guidelines, it certainly mentions all important considerations of transparent and rigorous reporting. 2.11.3 Data visualization Data visualization guidelines by Google. Google designers came together to construct guidelines for data visualization. The first product of this work is this collection of six principles on how to create meaningful and effective data visualizations. R Graph Gallery. A website that collects beautiful data visualizations with the code required to reproduce them. Code is available across a number of languages (e.g. R, Python, D3.JS). Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm. Are you a bioscientist that would like to produce good visualizations, has heard of the concerns about bar charts, but is unsure how to proceed? This PLOS Biology article is what you need. 2.11.4 Data analysis Statistical Thinking for the 21st Century. This is a book by our own Russell Poldrack that can be found for free online. It represents an excellent combination of modern statistical thinking, concise text meant for the non-expert and examples using R code. Regression and other stories. You are just bound to fit at least one regression in your lifetime as a researcher, and most for better or worse, are bound to fit hundreds of regressions. This is a new book fresh off the press and if we had to recommend one approachable book with someone with basic understanding of mathematics to learn from as a practical guide, this would be it. We love that they also make code available and part of the main text. Statistical Rethinking. If you have been hearing about Bayesian statistics and yearning for a simple guide to teach you the basics and give you ready-made code to plug and play, yearn no more. This is exactly what this book does and it does it really well. Computer Age Statistical Inference. Are you good with numbers, have a basic understanding of probability and statistics and want to learn how to shift towards incorporating more modern methods, such as the bootstrap? Search no more, this book by Stanford faculty does this better than anyone and it is available for free online. Introduction to statistical learning. Do you already have a basic understanding of probability and statistics and want a good book on how to start using methods of machine learning? This book by Stanford faculty is the best on the subject that we have encountered and one that we find ourselves returning to time after time. What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics. Have you heard about how many misinterpret the p-value but do not actually understand the concerns? Then this is the book for you. It is simple, no statistical knowledge is necessary to understand it and it does a fantastic job on explaining the p-value through the mentioned stories (it’s only ~200 pages). Introduction to Probability. Many complain that even though they have a working knowledge of statistics, they do not have a basic understanding of the underlying machinery. If you would like to read the one book that you will ever need to read about basic probability theory, this is it. It is phenomenally well written, provides the best explanations we have seen, extremely intuitive examples and a plethora of exercises, many with solutions. We have not even encountered a close second to this book. 2.11.5 Statistical analysis report HealthNewsReview.org. This website rates how well and accurately popular news articles describe the underlying findings. Even though specific to healthcare, this is a fantastic resource about common caveats to improve your critical thinking through. Unfortunately, this website is not as active anymore as it used to be due to funding issues, but all of their excellent work is still available and they do publish new content now and then. The Art of Statistics: Learning from Data. A fantastic book by arguably the leading communicator of statistics, David Spiegelhalter. As per the Amazon description: “How many trees are there on the planet? Do busier hospitals have higher survival rates? Why do old men have big ears? Spiegelhalter reveals the answers to these and many other questions - questions that can only be addressed using statistical science.” "],["publication-phase.html", "3 Publication phase 3.1 Making Data Available 3.2 Making Code Available 3.3 Reproducible Environments 3.4 Open Publication Models 3.5 Documenting Processes and Decisions 3.6 Additional Resources", " 3 Publication phase Although we typically think of a journal or conference paper as the key output of a research project, most research in fact generates a multitude of information, including data, code, study protocols, insights not included in the paper, and so on. Given the huge amount of work that goes into research, there is great value in making sure that as many of these outputs as possible are available to as many people as possible. We won’t make the argument here for why this is worth doing (though there are many good arguments for doing so; see Links below). Rather, this guide will describe best practices, and encourage you to adopt as many of them as possible. The five key areas that will be discussed in this section are: Open Data: making the raw data available for further research and replication Open Source Code: making the analysis pipeline transparent and available for others to borrow or verify Reproducible Environments: making not just the data and code available for others, but making it easy for them to re-run the analysis in an easily reproducible manner Open Publication Models such that anyone can see the scholarly output associated with the work Documenting Processes and Decisions: making it clear to interested parties not only what was done and how, but also why, by mechanisms such as open lab notebooks All of these are important, but each can be adopted independently of the others. Fortunately, there are now a huge range of tools and resources available for facilitating this work. At the end, we also provide some examples, and links to additional resources. 3.1 Making Data Available As one would expect for data science, the data itself is of central importance for transparent and reproducible research. Depending if you spend most of your time in a wet lab or a computer lab, you might have very different notions of what counts as data; here we will emphasize general principles and strategies that are widely applicable. 3.1.1 Vision for making data available The vision for sharing data is that, to the extent possible (while respecting concerns such as privacy and legality), researchers should make all data relevant to a project publicly available online. It should be stored in a repository that is accessible to all, that will endure over a long period of time, and in a format that is readable without commercial software; in many cases, the ideal is a human-readable format such as CSV or JSON. The data should be licensed and paired with documentation that explains what it is and how to use it. In most cases, the raw data should be shared (i.e., prior to any preprocessing, such as smoothing or outlier rejection), although it may also be helpful to share processed versions, along with details of what was done. Also make sure to provide something that people can cite to give you credit, either a DOI for the dataset, or a reference for the corresponding paper. 3.1.2 Essential considerations for sharing data Location online: Since the whole point of sharing data is to allow others to access it, it is obviously important that you put the data somewhere that is accessible to everyone and stable over time. You could, for example, choose to host the data on your own website, but this has severe disadvantages, including the possibility that your website will go down, and the inability of others to easily submit feedback. In practice, you are much better off putting it in a central repository, which offers numerous advantages, such as persistence and findability. Some good places to consider include: GitHub is very popular for sharing code and version control, with excellent archival practices. It can also be used for sharing data, but the maximum file size is on the small side (100Mb). https://github.com. Harvard Dataverse is free to all and allows 1 Tb of storage per researcher, with a maximum file size of 2.5 Gb: https://dataverse.harvard.edu/ Dryad is not free to all (typically used via institutional access) but allows for up to 300Gb per dataset: https://datadryad.org The Open Science Framework is a another comprehensive platform for open science, as well as a favourable place to make data publicly available: https://osf.io For a chart comparing several popular options, please see the Generalist Repository Comparison Chart Pro tip: For very large datasets (e.g., over 500Gb), or those that will be downloaded very frequently, you might want to consider paid alternatives, such as Amazon Web Services, which allow for a “requester pays” arrangement (e.g., https://arxiv.org/help/bulk_data_s3). Another option that could be considered would be to use torrents or some other distributed format, to help distribute the storage and transfer costs. Amount of preprocessing: Different people might want to use your data in different ways. For some, they will just want to incorporate your final numbers into their own work. Others might want to re-analyze your data. As such, different audiences will prefer different amounts of preprocessing. Generally speaking, it is recommended to include the “raw” data (that is, the data prior to any preprocessing, such as smoothing, compression, or deletion of outliers for tabular data, or tokenization for text data); however, because others may want to make use of the preprocessing you have done, it may also be valuable to include multiple copies at different stages of preprocessing. This is largely a matter of judgement, but connects to the sharing of code and reproducible environments (see below). Ideally, others should be able to easily reproduce all the preprocessing steps such that they can obtain any stage of data starting from the raw files. Documentation: Trying to make sense of someone else’s data can be extremely challenging. You can help them greatly by including a README file that explains what all the files are the repo are, how to load the data, and what the various fields correspond to; this may also connect with making code available (see below). Datasets for Datasets: One specific type of documentation that we suggest including is described in Datasheets for Datasets (https://arxiv.org/abs/1803.09010). This paper describes a series of questions that dataset developers should think about, as well as examples of datasheets developed for specific datasets. These questions and examples cover aspects such as who created this dataset and for what purpose? Are there known errors or problems with this data? Does it contain information that might be sensitive? How has it been used so far, and are there any potentially harmful uses, or applications that it should not be used for? We refer the reader to the original paper for further details, and encourage researchers to adapt the datasheet format to their specific needs. Format: It is important to make sure that your data is available in a format that will be useful to others. Good choices are formats that don’t require any specialized (paid) software, and that are robust across operating systems. For relatively simple datasets, such as tabular data, good choices include .csv and .json formats. For example, don’t share a Microsoft Excel file (.xlsx); export it as a .csv file, such that it can be opened by many programs. If your data requires a specialized data format (such as for neuroimaging data), make sure to document what the format is, and how others can load the data. Pro tip: comma-separate value files (csv) can cause difficulties when working with text data across platforms. JSON is a cross-platform framework that is flexible and human readable. Although it is slightly less compact, it can be worth it for ease of use. For a nice way to store tabular data, convert each row to a json object and write it as a string to a text file, one row per line. Also, be sure to specify a widely-used encoding format, preferably unicode (utf-8). – Dallas Pro tip: Be especially carefully when working with dates and times, as different programs may interpret this information differently. (In fact, the HUGO Gene Nomenclature Committee recently officially renamed several genes to avoid having them interpreted as dates by Microsoft Excel, e.g., MARCH1 became MARCHF1). One option is to explicitly break the dates down into the smallest meaningful unit (e.g., storing them with separate fields for year, month, and day). – Dallas License: Although we normally think of licenses as applying to code or other written materials, it is good practice to include a license that explicitly permits use by others with your dataset. Note that different licenses may be relevant to data as opposed to code. For datasets, the standard licenses differ primarily by whether they require attribution, whether they allow reuse for commercial purposes, and whether they require the same license be kept by derivative works. The number of choices can seem overwhelming, but there are many useful guides out there. An example license is the Creative Commons Zero license (CC0-1.0), which is higlhy permissive, but provides variations to specify particular restrictions (e.g., CC-BY, CC-NC). Harvard Datavese, for example, applies a CC0 license by default. To read further about licenses, refer to articles on data.world, the Open Knowledge Foundation, and UK Discovery. Updates: There may be a point at which you want to make a change to the data that you are sharing, such as when an error is found. As such, it is very useful to make use of version control, such that it is possible to look at the data from different versions. A big advantage of storing your data on a site like Github is that it makes such updates transparent, and allows users to navigate back to any earlier versions of the dataset. It also allows people to open “issues” if they run into problems, or want to suggest improvements or request updates. Whenever you do update the data, be sure to make absolutely clear what was changed and why. Also be sure to make it clear which version of the data was used in the official publication. 3.1.3 Challenges in making data available Legality: If you have collected data via a third party, it is essential to check their requirements for redistributing such data. For example, the Twitter terms of service state that researchers are not allowed to share any data about individual tweets; rather they are only allowed to share the tweet IDs. This means that anyone could in principle recollect the same dataset using these tweet IDs, but permits individual users to remove their tweets to prevent future collection. Note that this means that the dataset will not be truly stable over time. However, any alternative would violate the Twitter terms of service. It goes without saying that one should not make copyrighted material available without permission. Such considerations are also good to keep in mind when designing a study, and design data collection such that it will be possible to share data at the end of a project. Pro tip: When deciding whether or not data can be ethically shared, pay attention not just to the legal requirements, but also to the community norms. For example, research by Casey Fiesler and Nicholas Proferes found that Twitter users were largely unware that Twitter’s terms of service allowed their data to be used by researchers. (See also this medium post). – Dallas Privacy: In many cases, data may be sensitive and contain information that others would not want revealed. Typically this arises when one is working with data that have to do with people (demographic data, text data, etc.), although other reasons may arise (such as when working information that could be potentially dangerous if shared publicly). Although some people recommend “de-anonymizing” data by removing particularly sensitive information, such as addresses and social security numbers, researchers have now convincingly shown that almost any information could be sensitive in the proper context way. In particular, it turns out that any information might be identifying when combined with other external data. The classic example of this is the Netflix prize, in which many people’s movie viewing choices were made identifiable by combining this public data with other public data from IMDB. Pro tip: There is some exciting work being done on how to share private data without impinging on anyone’s privacy, including work in differential privacy. This space is still evolving however, so unless you are an expert or collaborating with an expert, it is better to err on the side of caution. You definitely don’t want to contribute to accidentally doxing someone online! – Dallas Backwards compatibility: Especially for data stored in obscure formats, there is a risk that researchers in the future may not be able to access the data because they are not able to run the software required to open it. While this may not seem like a pressing concern, software actually exists in a complex ecosystem. If you assume access to a particular version of a piece of software, it may not be available for future versions of the operating system. Although there are solutions here, such as the emulation techniques used to run old video games, this is another reason to favor formats that are simple and ideally “human readable” (meaning that one can simply look at the data in a text editor and it will make sense), such as .csv and .json. A note on sharing models: In many cases, it may also be useful or important to share trained models, either instead of or in addition to the raw data. Nearly all of the same considerations described above apply to sharing models as well. Indeed, it is important to remember that models can be thought of as a compressed representation of the data (along with prior assumptions and/or non-deliberate contributions, such as a random seed). This means that in some cases it may be possible to extract some or all of the original data from the model, and this should be kept in mind if the original data is sensitive or private. For additional recommendations regarding documentation to provide when sharing models, we recommend consulting Model Cards for Model Reporting (https://arxiv.org/abs/1810.03993). 3.1.4 Additional resources on data sharing For additional considerations, see the Primer on Data Management One popular framework for thinking about sharing data is FAIR (Findable, Accessible, Interoperable, Reusable). For more details on FAIR, please see resources on OpenAIRE. You can also search for more options on the Registry of Research Data Repositories. 3.2 Making Code Available Closely tied to sharing data is making the code to analyze that data available as well. Even in cases where the data itself cannot be shared (such as for reasons for legality or privacy), there is great value in still sharing whatever code you used to analyze that data. This not only allows others to see precisely how you analyzed it, it also makes your hard work available in a way that may be useful to others. Indeed, for complicated data analyses, it is extremely difficult to precisely describe every single step that was done, even with extensive supplementary material to a publication. As such, the actual code used to do the analysis is like a kind of self-documenting description of what was done (though please don’t skimp on the actual documentation!). While sharing data can be as simple (in the most minimal case) as just putting a single fine online, sharing code generally requires some additional care in order to deal with documentation, licesnses, dependencies, computational environments, etc. Bceause making code reproducible is so important, we are dedicating an entire chapter to it, complete with examples in python [FORTHCOMING]. 3.2.1 Vision for making code available The vision for sharing code depends to some extent on the nature of the project. For methodological work, this may entail releasing a package that can be used to perform a particular type of data analysis. For more substantively focused scientific research, on the other hand, the code shared might simply replicate the particular analysis presented in a paper, going from the raw data to the result obtained, without trying to generalize to other cases. In either case, the ideal outcome is that anyone can get access to the code, run it locally, and obtain the desired result, without excessive difficulty or cost. 3.2.2 Essential considerations for sharing code Begin with the end in mind: Sharing code ideally begins with adopting the mindset as early as possible that you will make your code available to others. Many people fear embarrassment if their code is not up to some standard, but in practice everyone very much appreciates when code is available, even when it’s messy (though clean and well-documented code is much preferable!) Methods and tools: A plan to make your code public will also inform your choice of methods. While it is technically possible to do statistical analysis in Excel, it is not really set up for reproducibility as a core element the way that something like R is. Python also nicely lends itself to reproducibility, when used with care. Other software systems such as STATA and Matlab may also provide reproducibility options, through the sharing of scripts, but are less preferred because they may not be freely available to all. Much may depend here on the complexity of the analysis involved. Pro tip: Both python and R provide extensive support (via libraries) for statistical analysis, visualization, interactive notebooks, optimization, and text processing. R still maintains a slight advantage when it comes to advanced but off-the-shelf statistical analyses (e.g. generalized hierarchical mixed effects models). Python on the other hand has better integration with deep learning frameworks, such as torch. Although many attempts have been made to make these two frameworks interoperable, these attempts to be somewhat brittle, and may go out of date quickly. – Dallas Consider the user: When deciding what to share, and how, think about who might want to use this code. If you are sharing a new method, it is likely someone who wants to apply this method to their own data. They will want to know how to install your software (if required), how to run it, what the options are, and will likely be happiest if you provide an example illustrating how it works. On the other hand, if you are sharing analysis code for replicating a result, people will likely want to re-run the exact same analysis you used, and perhaps try variations on it. They will want to know what is required to obtain the exact same results (e.g. precise versions of dependencies, access to the right data, etc.), and will also likely want to know the reasoning behind all choices that were made (making documentation all the more essential). In either case, it is good to assume that the user is less familiar with the project than you are, and will likely be confused unless you provide landmarks and explanations to help guide them. Location online: As with data, you need to decide where to store your code online, and as with data, there are multiple good options GitHub, which is owned by Microsoft, is by far the most popular site for sharing code, though many alternatives exist, such as BitBucket. Different alternatives often have different free or academic plans, which may be worth investigating. A major advantage of sites like GitHub is the integration of code, data, versioning, and “issues”, such that others can post questions, suggest improvements, or submit updates to be incorporated into the main branch. Using the full potential of git can be quite complicated, but the basic usage of creating and uploading code to a repo is actually quite straightforward. For an introduction to sharing code via GitHub, we recommend the relevant chapter of The Turing Way. CodaLab is a Stanford initiative that allows sharing code and data in the form of “worksheets”, which are like “executable papers”. Many additional alternatives exist (see sites listed under data sharing 2.2.1 for more) License: It is important to include a license which covers the legal usage of your code. Several open-software licenses have become more or less standard, and are all quite similar, though differ in a few key ways. The MIT license is one of the most permissive, and allows repurposing your code wihtout attribution, even for commercial purposes. The Apache 2.0 license is similar, but places more explicit restrictions on trademarking or patenting work dervied from your code. The GNU GPLv3 license is more restictive, and requires that new distributions of your code must document changes, make the soucre code public, and be released using the same license. GitHub provides standard template licenses for these and other types. For more advice on choosing a license, please refer to the Open Source Initiative, choosealicense.com or the relevant chapter of The Turing Way. Obviously you should also pay attention to the licesnses attached to any code you incorporate into your own work, which may dicatate which license you have to use! Documentation: As with sharing data, documentation is essential. In addition to an overall README file that explains what the project is, provides a link to the paper, etc., it is good to provide comments in each file to explain as much of the code as possible. Typical documentation should include: A brief description of what this repository is; Installation instructions that anyone can follow; A list of required dependencies (see below) with version numbers (e.g., in a requirements.txt for python); Description of how the software can be used; A brief (working) illustrative example, complete with an example dataset; A reference to any accompanying publications or related resources. 3.2.3 Challenges in sharing code Software engineering: There is a reason that some people specialize in software engineering – there is an enormous amount that is known about how to develop software. Although much of it could be relevant to writing and sharing research code, it is also possible to focus on the parts that matter most. For those who are interested, The Turing Way again provides some good starting points. Data that cannot be shared: Even if the data itself cannot be shared, there is still great value in sharing the code, as this will help others to be able to see exactly what was done; in this case, it may be very helpful to provide an artificial dataset that is similar in structure to the true data, to help others to be able to easily run your code. This can also be a useful tool in debugging your analysis – to create a simulated dataset that mimics many of the features of your real data, but with known parameters. For starting points on this approach, see this blog post from Andrew Gelman. Hyperparameters: As advabced methods from machine learning are being more widely adopted, issues related to hyperparameter selection have taken on a new urgency. For some methods, such as ridge regression, the main hyperparameter might be something as simple as regularization strength. For modern deep learning architectures, on the other hand, the number of hyperparameters is virtually limitless. If you just want people to be able to replicate an analysis, it may be sufficient to provide the exact hyperparameters used (including a random seed which guarantees reproducible results, though see below). Much better, however, is to provide some rationale for why these values were chosen, or even code to reproduce the selection (e.g. by running random search). This is a rich topic which we explore in the analysis section. Pro tip: What is a hyperparameter? There is no single definition that covers all situations. Conceptually, it is some part of the specification of a model that could reasonably take on multiple different values (such as regularization strength). In practice, we generally think of it as something we might tune in order to try to improve the model (including the learning rate or other parameters of an optimization algorithm, or even the optimization algorithm itself!). In theory, as Maclaurin, Duvenaud, and Adams have pointed out, even the dataset itself can be thought of as a kind of hyperparameter, though, as they note, this risks a kind of “philosophical vertigo”. – Dallas Dependencies: Almost all code will require some sort of dependencies in order to run. For example, an R script requires that the user have R installed. Although environments such as R are relatively stable over time, there are occasionally updates to environments and packages that break backwards compatibility, especially for modules under active development, such as Torch and Tensorflow. It is highly recommended to provide a list of all dependencies used (e.g. packages in python or libraries in R), and the exact versions you used in running your analysis. For python, this could take the form of a requirements file, which lists the version of each package in the environment (e.g., tensorflow==0.12). Pro tip: Anaconda is a great option for python which provides both a package manager, named “conda”, and a basic slate of packages for scientific computing. Just like pip and virtualenv, conda allows you to create a new environment for your project, install relevant packages to that environment, and then export that environment in a way that others can easily recreate. For an introduction to conda see this introduction or the conda documentation. – Dallas Computing environment: In some cases, even providing the exact dependencies will not be sufficient, as results can depend even on hardware considerations, such as which GPU was used. At a minimum, it is good to document such choices if they might be relevant, and we provide more detail on this in the Reproducible Environments section below. Randomness: Although many simple analyses may be purely deterministic, for a lot of work related to machine learning, there are many ways in which randomness can creep into the code. It can come in through the initialization of weights in neural networks, in partitioning the data into train and validation data or in shuffling the data, as well as other considerations. Although there is great value in exploring how your results might vary across this randomness, it is also good to make sure that it is possible to reproduce one set of results exactly. This is typically done by setting a random seed and sharing it. Note however, that this can sometimes be tricky. For example, in python, you might need to set a seed for both the “random” package and “numpy” (if using both) and/or make sure these get propagated to other frameworks such as torch. It’s always good to check to see if others are able to reproduce your results exactly using different environments. Pro tip: Recent work has shown that both the random seed and the ordering of data can make a massive difference to the performance of complicated NLP models such as BERT. When working with deep learning, it is worth thinking about whether you are making a claim about a particular instantiation of a particular instantiation of a trained model, or about the expected performance of a family of models. The type of claim you want to make should dictate the type of experiments you choose to run. Above all, make sure to show your work! – Dallas 3.3 Reproducible Environments Even with the most beautifully written code, users will not necessarily be able to successfully run it and obtain an identical result unless they can recreate the same computing environment that was used. In some cases, the version of a software package can make a big difference to the outcome. In other cases, even the computer hardware that was used can matter. While recording the versions of all packages used is a step in the right direction, an even more comprehensive solution is to package up the entire environment using something like Docker, or to use online computation, such as Google’s colaboratory notebooks. 3.3.1 Vision for reproducible environments For any code that is shared, authors should make it as easy as possible for others to run it and obtain the same result. This includes availability of software, path dependencies, software dependencies, and general computing environment. Ideally, this process should involve testing to make sure that others are able to recreate one’s results and/or a system for users to report errors that they encounter. 3.3.2 Essential considerations for reproducible environments Software availability: The most obvious consideration for reproducible environments is the software that is used. If programs are written in commercial packages, like SPSS, Matlab, or Tableau, this means that only researchers with access to those packages will be able to use the code that is shared. Much better is to use freely available tools, like python, R, or other mainstream programming languages. Path dependencies: During software development, it is often convenient to write explicit file locations into the code. However, this will not work for others who have files in different locations. For example, if you tell your script to load a file at /Users//data/ then others running that script will encounter a problem. One option is to use relative path locations. For example, if you have a “data” directory and a “scripts” directory, you might have scripts load data from “../data/”. Alternatively, you can make use of command-line arguments and/or a configuration file that users can easily modify to customize things for their system. Providing an example configuration file and allow uses to modify it is generally much better than encoding things into the code itself. Pro tip: Note that the way of specifying paths differs between different operating systems, such as Mac/Linux and Windows. One option to avoid this problem is to use system-agnostic tools to specify paths, such as python’s os.path.join() function. – Dallas Package dependencies: Although we often think of it as relatively static, software is constantly changing and being updated. Even packages of central importance to researchers, such as python’s numpy are constantly being upgraded with new features and bug fixes. Many of these can be benign, but some can be “breaking” changes, such that people using older versions will no longer obtain the same result. Fortunately, most older versions of packages remain available. So, a minimal solution is to record the packages that you used when running your experiments. Others can then use this information to create the same environment. With python, you can do this by developing and running your code in a project environment, using either pip or conda, and exporting that environment as a requirements file. Pro tip: Python 2 has now reached the end of it’s life. If you are creating a new project in python, make sure to use Python 3. – Dallas Containers: An even more comprehensive solution is to use container systems, such as Docker. These solutions package up an entire computing environment in one file, such that anyone can easily run things in the exact same environment you used. Although not as widely used as they could be, it is likely we will see this sort of solution become the norm over time. For a good introduction to containers, see The Turing Way. Online environments: Another alternative is to not have others use their local resources at all. This can be done, for example, by hosting your code in the cloud, either using something like Amazon Web Services, or a simpler solution, such as an interactive notebook shared through Google Colaboratory. Although there are open questions about how stable these will be over time, they do have the advantage that anyone can easily run your code without doing anything more than clicking in a web browser. Pro tip: At the time of writing, Google Collaboratory even provides limited free access to GPUs! – Dallas 3.3.3 Challenges in reproducible environments Deprecated software: Software has a tendency to go stale. In the worst cases, the tools you used may no longer be available after some period of time. For example, many machine learning papers were published based on code written using theano or early versions of tensorflow, and reproducing these results is now much more difficult as a result. The risk of this can generally be minimized by using mainstream, well-supported packages with active communities, such as python’s numpy or scikit-learn. The risk is generally highest when working at the cutting edge. Maintenance: In some ways, the largest cost associated with sharing code or releasing software is that people will want you to help solve their problems! Many potential problems can be headed off by following the advice above (e.g. providing requirements files, etc.). However, if your code is popular, people may have requests for features they would like you to incorporate. Someone may even discover a bug. Fortunately, there are also good options here for migrating your code to a real open-source project. In particular, GitHub provides a number of features beyond just archiving and making your code available. It allows others to “fork” your repository, making their own copy of it which they can modify, while leaving your original intact. Anyone can “open an issue”, if they discover a problem or want to request an extension, and the GitHub system organizes these. Users can even submit “pull requests”, in which they are asking for changes they have made to be incorporated into your main codebase. While these advanced features are unlikely to apply to most projects, as usual, it is worth it to begin with the end in mind. Hosting your code on GitHub doesn’t require that you have any knowledge of these more advanced features, and easily allows your project to expand as necessary going forward. Custom vs generic solutions: There are now multiple systems designed to facilitate the sharing of research materials, including runnable code, such as Binder, and the eLife Reproducible Document Stack. At this point, there is no one solution that will be best for all researchers, and some work may be required to determine which, if any, will work for you. However, we would also warn that there is some risk in attaching one’s research to a particular platform, which may or may not persist over the long term. Where possible, we advocate for keeping things simple – transparent, well documented code, raw data in a human-readable format, careful specification of dependencies, and storage in one or more well-supported locations (e.g. GitHub). For a review of the current landscape of infrastructure to support open and reproducible research, please see this paper. 3.4 Open Publication Models Academic publishing is a complex ecosystem with a lot of moving parts. While the key role is still the dissemination of ideas, published papers accomplish many things, from providing focal points for discussion, to helping people get promotions. While no one system can adequately serve all these purposes equally well, it is especially important to keep the first of these in mind, and try to make your research available to as many people as possible. Regrettably, there are serious hurdles to this, but as in all areas, new innovations are shaking up this system and providing new and better options. 3.4.1 Vision for open publication models The vision for open publication is simple: any paper you publish should be freely available to anyone on Earth. Unfortunately, this is not the norm; most papers end up in journals that can only be accessed by people with extremely expensive subscriptions (or a pay-per-article model that basically no one participates in). There are many practical and philosophical arguments behind this vision, but in line with our emphasis on the practical, we simply maintain that this is the right thing to do. In practice, there is a spectrum of options, which we describe below. 3.4.2 Publishing models Traditional publishing: The traditional publishing model is the one we want to avoid. In this model, journals select which articles they will publish, and then only make those articles available to people who pay for access (typically through a university library). This may serve the authors, who can list the paper on their resume, and it may to some extent serve the community, as most researchers working in the area will likely have access through their universities. But it contributes to a broken system, and is no longer strictly necessary. Many good alternatives now exist, and the only hurdle to moving away from this system is the collective action problem of needing entire communities to make the move together. “Open-access” publishing: The term “open-access” can have many meanings. When used by traditional publishers, however, it typically means a model by which authors pay additional fees, and publishers make the published paper freely available online. This is also the default model used by certain modern journals, such as Frontiers. Where possible (i.e. for researchers that have the funds available), this is preferable to the traditional model, though it is by no means the only one that should be considered. A Directory of open access journals can be found at: https://doaj.org/ Preprints: Many journals will force people to pay for access to their journals, but still allow authors to share a “pre-print” or other copy of the paper on their website. If you are publishing in a paid journal, you should check to see what the rules are. If it is allowed, you can make a copy of your article freely available on your website (along with a link to the data and code!) Preprint servers: One of the most exciting revolutions in science in recent decades has been the rise of preprint servers, such as arXiv.org. These systems allow anyone to upload papers, adding them to the permanent archive. arXiv.org also emails those who subscribe with daily updates about new papers that have been uploaded in each area. These systems receive support from various sources, such as universities and foundations, and allow anyone to upload a paper or read any paper they host, all for free. The downside is that preprint servers do not provide a peer review process or any space for comments from the research community (and so papers that are only published on arxiv may be seen as having less worth or credibility). Pro tip: People sometimes speak of peer review as the “gold standard” for scientific publications, but it is important to remember that peer review is not a guarantee of quality, and entails its own biases. Peer review can help, in some cases, to provide useful feedback to authors or to filter out very low-quality work, but we should also not overrate its value or underestimate its downsides. – Dallas Conference models: In some fields, such as computer science, conference publications as an important pillar exist alongside traditional journals. Details may differ, but the standard setup is that costs are covered by membership dues (paid by those who attend the conferences), and reviews are carried out by the community. As such, these conference publications are treated as interchangeable with journal publications for the purposes of promotion. The key advantage is that there is no cost to publish, and the papers are freely available to all. This is the model used by conferences such as NeurIPS and ACL. This is in many ways the optimal model, though it is unclear how easy it is to create such a system and make it sustainable. Experimental models: Yet more systems are constantly being created and tried. New online journals have been started specifically to focus on null results and replications. Various comment sites exist, which allow people to have online dialogues about published work. Sites such as Distill.pub have embraced interactive papers, which embed animations or interactive widgets. Others have experimented with embedding data directly into papers in various ways. None is as robustly developed as any of the above systems yet, but they serve as an excellent reminder that we are not restricted to traditional models. The sky is the limit and anything is possible here! Pro tip: For an excellent review of interactive publication models, have a look at this recent Distill.pub article. – Dallas 3.5 Documenting Processes and Decisions Every data analysis involves decisions. Reporting the choices that were made is a big step towards reproducibility. However, this alone does not necessarily tell readers why those choices were made. For example, reporting the full set of hyperparameter values used will better enable others to reproduce your results, but it is also important to know how you arrived at those particular values (even if it was as simple as prior intuition). As a last section of best practice in reporting, we encourage readers to be transparent and open about the processes and decisions made in carrying out their research. 3.5.1 Vision for documenting processes and decisions It is likely impossible to entirely document and/or recreate all of your decision making processes. However, to the extent possible, it would be ideal to create a supplementary document that describes the process by which key decisions were made. This might describe the process of model selection, or hyperparameter tuning. It might describe preliminary data analyses or pilot experiments. It might explain how you arrived at a particular criterion for removal of outliers, and so on. In the most fully-formed case, you can think of this as a kind of “lab notebook” for data science work. Having a record of how you arrived at various decisions might be important in the future, both for yourself and others! 3.5.2 Examples of documenting processes and decisions Pilot studies: It is common in many fields to run a small pilot experiment before running the full experiment. This might be to verify that a protocol works, or to estimate power. It is rarely worth describing every aspect of this in the main paper, but it is important to document somewhere what the results were and how they informed the main study. Study protocol or codebook: In annotation projects, it is common for procedures to evolve slightly as the data collection proceeds. A codebook describes the guidelines that are given to annotators in annotating data, and describes overall goals, as well as important clarifications for ambiguities that may arise. It is important to make such codebooks public, so that others can attempt to follow the same protocol, as well as document key moments in which the codebook was updated. Model selection: An infinite range of are possible for any particular dataset. Often the reasons for using a particular model are obscure. Sometimes there is an “obvious” or “default” model to use (e.g. linear or logistic regression). In other cases, we might try many models and choose one based on how well it fits the data, or just a validation set. This kind of choice is particularly important to report, as it may drastically influence the results. Readers should be aware if you tried many models and only reported what seemed to be the best. Alternatively, if there is some good reason for using a particular model, that is also useful for interested readers to know. Hyperparameter tuning: Especially for machine learning models, there are often many hyperparameters that can be tuned for better performance (e.g. regularization strength, learning rate in optimization, etc.). Even the model itself can be thought of as a kind of hyperparameter, as can the random seed used to initialize the model and/or sort the data. There are also many processes for selecting hyperparameters, from random search, to grid search, to more sophisticated iterative methods. The more detail provided here the better. When extensive hyperparameter tuning is done, it may also be valuable to report expected validation performance (see arxiv.org/pdf/1909.03004.pdf). Retrospectives: The idea of doing a retrospective analysis of published work is gaining popularity in machine learning. This might include revisiting assumptions, discussing limitations, or looking at things from a new perspective. For more examples, see ML Retrospectives. 3.6 Additional Resources The Turing Way: An excellent community-driven guie to reproducible data science. The Center for Open Science: A central organization devoted to open and reproducible research. The Open Source Initiative: Working to raise awareness and adoption of open source software. "]]
